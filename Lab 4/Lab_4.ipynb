{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6580a684-d54a-4f48-b0a0-496313f79b43",
   "metadata": {
    "id": "6580a684-d54a-4f48-b0a0-496313f79b43"
   },
   "source": [
    "# Lab 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4tcoIJnMc17C",
   "metadata": {
    "id": "4tcoIJnMc17C"
   },
   "source": [
    "In this lab,  we will be looking at\n",
    "\n",
    "1. Pre-processing, tokenizing text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc0790-0b1c-48ae-ba42-b3ff7f619708",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:39:58.949786Z",
     "start_time": "2022-09-06T09:39:58.930778Z"
    },
    "id": "72fc0790-0b1c-48ae-ba42-b3ff7f619708"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import lzma\n",
    "import json\n",
    "\n",
    "import re\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from config import settings_base as settings\n",
    "from config import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f9f219-05ad-47b6-b4bc-7725a6ef6255",
   "metadata": {
    "id": "77f9f219-05ad-47b6-b4bc-7725a6ef6255"
   },
   "source": [
    "# Part 1 -Text pre-processing, tokenization, featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8600705d",
   "metadata": {},
   "source": [
    "##  Tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfed5632",
   "metadata": {},
   "source": [
    "As you might've noticed, the text data we are dealing with is mostly **strings**. There are a couple of problems with this:\n",
    "\n",
    "* As you can imagine, a computer is not really able to \"understand\" what a \"word\" (or token). This is because in a \"string\", a whitespace is just another substring. To a computer, all letters and spaces are meaningless - it's just information to store.\n",
    "* Thus, in order to ensure that we can parse the string, we must first identify the \"words\" (or tokens) in it. This is called [**tokenization**](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization)\n",
    "* In simplistic terms, tokenization is when we're trying to put some units of text (whether words or sentences) as items in a list.\n",
    "* Now that we know what the purpose of tokenization is, we can read the wikpiedia definition: \"Tokenization is the process of demarcating and possibly classifying sections of a string of input characters. The resulting tokens are then passed on to some other form of processing. The process can be considered a sub-task of parsing input.\"\n",
    "* Furthermore, tokenization is usually an important step __prior__ to any text analysis that we do. For example, last Lab, we saw how \"Phrasemachine\" __only works with tokenized text__. \n",
    "* Thus, tokenization can also be thought of as one of the first steps on __pre-processing__ text - ie trying to find words in an unstructured string. \n",
    "* This goes back to the data structure distinction (between strings and lists for example) that we covered in Lab 2. Some libraries can work with strings, but others require tokenization (a list of words) as input. When we think about it, this is just another way of and __inputting__ text into some function. \n",
    "* \n",
    "\n",
    "To understand tokenization better, let's try a simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c99009b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:29:10.912177Z",
     "start_time": "2022-09-06T09:29:10.904163Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_string = \"The quick brown fox jumps over the lazy dog.\"\n",
    "sample_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea63a8a",
   "metadata": {},
   "source": [
    " Now let's lowercase this string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45e930c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:29:12.775230Z",
     "start_time": "2022-09-06T09:29:12.766244Z"
    }
   },
   "outputs": [],
   "source": [
    "text_lower = sample_string.lower() # lowercase \n",
    "text_lower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350d0eea",
   "metadata": {},
   "source": [
    "The simplest heuristic we can apply to tokenization is to use **\"whitespace tokenizsation\"** - meaning that every time a computer sees a whitespace, it will split the string. Let's try that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7ea167",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:29:14.395176Z",
     "start_time": "2022-09-06T09:29:14.385173Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokens\n",
    "tokens = text_lower.split() # splits a string on white space\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e43f91",
   "metadata": {},
   "source": [
    "One of the problems with this simplistic approach is that punctuations can be preserved as parts of tokens. Note the last word - \"dog.\" Thus, the word is not \"dog\" but \"dog.\" - even though to humans this might not seem like a big difference, but to a computer these two words are completely different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ab3f55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:29:16.351168Z",
     "start_time": "2022-09-06T09:29:16.335161Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens[-1] # the last element of the list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6524c99",
   "metadata": {},
   "source": [
    "**Thus, we can try to remove punctuation entirely.** But it's important to note that we will be losing some important information - and ideally we want to preserve as much as possible (but it also depends!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad8a134",
   "metadata": {},
   "source": [
    "There are many ways of removing punctuation. For example, we can use a Regular Expression tokenizer which matches a pattern which captures **only words** - click on this [regex pattern](https://regexr.com/6sfat). Note that because we match only words (**\\w+**), the dot is not captured by the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2251c809",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:29:21.383533Z",
     "start_time": "2022-09-06T09:29:20.455026Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenizer.tokenize(text_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cba0c7",
   "metadata": {},
   "source": [
    "We can also use libraries for this - for instance, gensim. But the key point is there are many ways of doing these things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80440a96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:29:23.508253Z",
     "start_time": "2022-09-06T09:29:23.173791Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "\n",
    "no_punctuation = strip_punctuation(text_lower)\n",
    "no_punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4a8688",
   "metadata": {},
   "source": [
    "As you can see, the string doesn't have a dot. Now we can again use the simple \"split\" syntax to split on whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfeaf13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:29:25.932390Z",
     "start_time": "2022-09-06T09:29:25.921403Z"
    }
   },
   "outputs": [],
   "source": [
    "no_punctuation.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c543e223",
   "metadata": {},
   "source": [
    "What about numbers? Let's change the string a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2374a9e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:29:28.866409Z",
     "start_time": "2022-09-06T09:29:28.862428Z"
    }
   },
   "outputs": [],
   "source": [
    "text_with_numbers = \"these 2 quick brown foxes jump over those 20 lazy dogs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f3f70e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:29:29.744341Z",
     "start_time": "2022-09-06T09:29:29.732339Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = text_with_numbers.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7097ac7e",
   "metadata": {},
   "source": [
    "Below we'll use a [list comprehension](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions) which gives a convenient way of creating a list out of an existing list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f1ea2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:30:37.688809Z",
     "start_time": "2022-09-06T09:30:37.669817Z"
    }
   },
   "outputs": [],
   "source": [
    "# Numbers\n",
    "# remove numbers (keep if not a digit)\n",
    "# Here - we're using a \"list comprehension\"\n",
    "no_numbers = [t for t in tokens if not t.isdigit()]\n",
    "print(no_numbers )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7a74b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:30:40.049103Z",
     "start_time": "2022-09-06T09:30:40.035015Z"
    }
   },
   "outputs": [],
   "source": [
    "# keep if not a digit, else replace with \"#\"\n",
    "norm_numbers = [t if not t.isdigit() else '#' \n",
    "                for t in tokens ]\n",
    "print(norm_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3bee79",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6280c15e",
   "metadata": {},
   "source": [
    "Another key concept that comes up when we deal with pre-processing is the so-called \"stopwords\" removal - words like \"the\", \"and\" etc. A word like \"the\" for example has no fundamental semantic meaning other than being a grammatical definite article.\n",
    "\n",
    "But note - **just because we remove the word *the*, doesn't mean it's not informative.** In fact, many of these stop words are incredibly important, but for the purposes of simple counting and calculations, they can be removed, at least, in theory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a337df18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:30:46.625741Z",
     "start_time": "2022-09-06T09:30:46.552732Z"
    }
   },
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f664547f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:30:46.625741Z",
     "start_time": "2022-09-06T09:30:46.552732Z"
    }
   },
   "outputs": [],
   "source": [
    "stoplist = stopwords.words('english') \n",
    "print (\"stop words:\", stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6e190c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:31:05.801859Z",
     "start_time": "2022-09-06T09:31:05.796869Z"
    }
   },
   "outputs": [],
   "source": [
    "# keep the words if not a stopword\n",
    "nostop = [t for t in tokens if t not in stoplist] # list comprehension\n",
    "print(nostop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed9418f",
   "metadata": {},
   "source": [
    "There are different packages with different stopword lists. \n",
    "\n",
    "Here's an example from **scikit-learn**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464a5062",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:31:07.797591Z",
     "start_time": "2022-09-06T09:31:07.791596Z"
    }
   },
   "outputs": [],
   "source": [
    "# scikit-learn stopwords\n",
    "# depending on sklearn version, for sklearn==0.24.1, stop_words are here\n",
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS as stop_words\n",
    "print(sorted(list(stop_words))[:20]) #first 20 stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3432c8",
   "metadata": {},
   "source": [
    "**Gensim** stopwords removing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f98dcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:31:09.974320Z",
     "start_time": "2022-09-06T09:31:09.957063Z"
    }
   },
   "outputs": [],
   "source": [
    "# gensim stopwords\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string\n",
    "\n",
    "remove_stopwords(\"Better late than never, but better never late.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3301aa",
   "metadata": {},
   "source": [
    "**Spacy** stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb0215d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:31:15.919012Z",
     "start_time": "2022-09-06T09:31:11.006251Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfe5c85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:31:15.935011Z",
     "start_time": "2022-09-06T09:31:15.920012Z"
    }
   },
   "outputs": [],
   "source": [
    "print(sorted(list(nlp.Defaults.stop_words))[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b27b4ba",
   "metadata": {},
   "source": [
    "For the more CS-minded folks, stopword removal can also be thought of as __\"dimensionality reduction\"__ - more on this in the next lab. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8985d4",
   "metadata": {},
   "source": [
    "## Stemming "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abccd7dd",
   "metadata": {},
   "source": [
    "Stemming is reducing a word to its stem or root. For example, according to the [SnowballStemmer documentation](https://pypi.org/project/snowballstemmer/), \"the English stemmer maps \"connection\", \"connections\", \"connective\", \"connected\", and \"connecting\" to **connect**.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cee507c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:31:23.190970Z",
     "start_time": "2022-09-06T09:31:23.172961Z"
    }
   },
   "outputs": [],
   "source": [
    "## recall our token list\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cfc8c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:31:24.350253Z",
     "start_time": "2022-09-06T09:31:24.336269Z"
    }
   },
   "outputs": [],
   "source": [
    "# Stemming\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english') # snowball stemmer, english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e956ffc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:31:28.563740Z",
     "start_time": "2022-09-06T09:31:28.553642Z"
    }
   },
   "outputs": [],
   "source": [
    "# remake list of tokens, replace with stemmed versions\n",
    "tokens_stemmed = [stemmer.stem(t) for t in tokens]\n",
    "print(\"Unstemmed tokens: \\n\",tokens)\n",
    "print(\"Stemmed tokens: \\n\", tokens_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0513cc69",
   "metadata": {},
   "source": [
    "## Lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c65c824",
   "metadata": {},
   "source": [
    "Lemmatizing is closely related to stemming, with the difference being that we reduce the word to its \"lemma\" rather than its \"stem.\" A lemma is a more fundamental base form of a word. The difference is subtle, but a good example from wikipedia is:\n",
    "\n",
    "* The word **\"better\"** has **\"good\"** as its lemma. \n",
    "* [Spacy](https://spacy.io/usage/spacy-101#features) gives the following examples: Assigning the base forms of words. For example, the lemma of **“are”** is **“be”**, and the lemma of **“rats”** is **“rat”.**\n",
    "* This link is missed by stemming - lemmatizing requires knowledge about the __Part of Speech__ tags of the word and the words around the word (whether a word is surrounded by Nouns or Verbs), and is thus a bit more intricate than stemming. \n",
    "* Therefore, lemmatization is related to stemming in that they are both methods for **normalizing text** - ie making the text more \"standard\", where it doesn't matter that you use \"are\" or \"is\" - both are reduced to \"be.\"\n",
    "* Depending on the context, one can prefer to lemmatize or stem their text (but usually, lemmatization is preferred - but again, it depends) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4b426e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:31:35.876755Z",
     "start_time": "2022-09-06T09:31:35.860735Z"
    }
   },
   "outputs": [],
   "source": [
    "text_lower = 'these 2 brown foxes are better than 20 dogs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c6973b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:31:36.409533Z",
     "start_time": "2022-09-06T09:31:36.396540Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(text_lower)\n",
    "\n",
    "for token in doc:\n",
    "    print(token, token.pos_, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb9b3c0",
   "metadata": {},
   "source": [
    "Note: Keep in mind that POS tagging itself is not ideal, as the example below shows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf52b03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:31:39.560489Z",
     "start_time": "2022-09-06T09:31:39.555490Z"
    }
   },
   "outputs": [],
   "source": [
    "text_lower = 'hurt people hurt people'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f0000d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:31:39.984301Z",
     "start_time": "2022-09-06T09:31:39.962321Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(text_lower)\n",
    "\n",
    "for token in doc:\n",
    "    print(token, token.pos_, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b2648e",
   "metadata": {},
   "source": [
    "## Sentence tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94598528",
   "metadata": {},
   "source": [
    "Tokenizing sentences is just like tokenizing words - the difference being is that each item in a list is going to be a sentence rather than a token. This is useful - sometimes, for your research purposes, the __unit of analysis__ might be sentences and not individual words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884dbc4e",
   "metadata": {},
   "source": [
    "Let's modify our running example a little bit by adding a quote from David Foster Wallace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bfe2c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:31:54.604631Z",
     "start_time": "2022-09-06T09:31:54.590636Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog. And yet: why not be someone who stays up all night torturing himself mentally over the question of whether or not there's a dog?\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64f80a9",
   "metadata": {},
   "source": [
    "Let's try nltk's sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d83e0b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:32:05.041646Z",
     "start_time": "2022-09-06T09:32:05.024653Z"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a727cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:32:07.384771Z",
     "start_time": "2022-09-06T09:32:07.361782Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text) ### sentence tokenization\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9517b0d8",
   "metadata": {},
   "source": [
    "Now let's try spacy's sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d055dde7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:32:08.670318Z",
     "start_time": "2022-09-06T09:32:08.646316Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "sentences = list(doc.sents)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ca4dc4",
   "metadata": {},
   "source": [
    "Although the outputs are the same, what's going on behind the scenes is not. \n",
    "You should note that the `spacy` sentence tokenization is based on a nlp model (which we call using `nlp()` syntax - and this model takes into account more information that `nltk`'s sentence tokenizer.\n",
    "\n",
    "* Thus, spacy be good if you want good quality sentences\n",
    "* But since it's a bit more complex, it's computationally more expensive, and thus could take longer than nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b6309e-4f76-4a3b-9423-bc3f7a6505cd",
   "metadata": {
    "id": "b4b6309e-4f76-4a3b-9423-bc3f7a6505cd"
   },
   "source": [
    "# Part 2 - Comparing texts - exploratory data analysis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In this lab, we will be investigating cases from the state of **Delaware**. \n",
    "\n",
    "As we will find out, Delaware is a pretty interesting jurisdiction when it comes to US Law - and we will try to demonstrate why Delaware is so unique in this lab using text comparison techniques.\n",
    "\n",
    "To extract cases, we will again use the case.Law API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a538e2-4d99-498e-9685-21c03ffe266f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:34:08.999983Z",
     "start_time": "2022-09-06T09:34:08.984984Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2a538e2-4d99-498e-9685-21c03ffe266f",
    "outputId": "003ce789-fce6-478e-c120-af5a13dd29dd"
   },
   "outputs": [],
   "source": [
    "compressed_file = utils.get_and_extract_from_bulk(jurisdiction = \"Delaware\", \n",
    "                                                  data_format=\"json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d244e8a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:34:31.968689Z",
     "start_time": "2022-09-06T09:34:26.296617Z"
    }
   },
   "outputs": [],
   "source": [
    "#If you downloaded the file you can also use the following code:\n",
    "compressed_file = utils.get_cases_from_bulk(jurisdiction=\"Delaware\", data_format=\"json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bd6ff3-81a1-46df-a335-b5f1177b374e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:34:37.692589Z",
     "start_time": "2022-09-06T09:34:33.057601Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "44bd6ff3-81a1-46df-a335-b5f1177b374e",
    "outputId": "a1888880-8a31-4562-f76c-94a8c686ff81",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cases = []\n",
    "print(\"File path:\", compressed_file)\n",
    "with lzma.open(compressed_file) as infile:\n",
    "    for line in infile:\n",
    "        record = json.loads(str(line, 'utf-8'))\n",
    "        cases.append(record)\n",
    "\n",
    "print(\"Case count: %s\" % len(cases))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3517016d",
   "metadata": {},
   "source": [
    "Again, if you want to limit the amount of cases you look at, you can use the code from last lab to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f14a876-5b7e-4d25-83d6-87b2e4f060b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:34:44.048286Z",
     "start_time": "2022-09-06T09:34:43.957265Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 687
    },
    "id": "2f14a876-5b7e-4d25-83d6-87b2e4f060b0",
    "outputId": "6636a55b-976c-498d-c31d-2e8fe2f508bf"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(cases)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31314f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:35:59.803669Z",
     "start_time": "2022-09-06T09:35:59.777982Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[['name_abbreviation', 'decision_date', 'court', 'casebody' ]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a436d4c6-84b8-496e-b126-e9957cad29bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:36:18.042620Z",
     "start_time": "2022-09-06T09:36:18.032622Z"
    },
    "id": "a436d4c6-84b8-496e-b126-e9957cad29bd"
   },
   "outputs": [],
   "source": [
    "# Helper function to extract the main text in casebody column\n",
    "def get_text(x):\n",
    "    if len(x['data']['opinions'])>0:\n",
    "        return x['data']['opinions'][0]['text']\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Helper function to remove punctuations in text \n",
    "def remove_punctuation(txt):\n",
    "    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \n",
    "                           \" \", \n",
    "                           txt).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50c7c89",
   "metadata": {},
   "source": [
    "In the cells below, we will create a a new column called __\"court_name\"__ which will indicate the name of the Court in the State of Delaware. We are particularly interested in comparing words between the **\"Delaware Court of Chancery\"** and the **\"Delaware Court of Common Pleas\"**\n",
    "\n",
    "* According to the website description of their jurisdiction, the [**\"Delaware Court of Common Pleas\"**](https://courts.delaware.gov/commonpleas/jurisdiction.aspx) deals with both criminal and civil issues. You can read the details of cases it hears on their website.\n",
    "* On the other hand, the [**\"Delaware Court of Common Pleas\"**](https://courts.delaware.gov/chancery/jurisdiction.aspx) website states that \"The Delaware Court of Chancery is widely recognized as the nation's preeminent forum for the determination of disputes involving the internal affairs of the thousands upon thousands of Delaware corporations and other business entities through which a vast amount of the world's commercial affairs is conducted. Its unique competence in and exposure to issues of business law are unmatched\" - this actually interestingly sounds like an advertisement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8205fee4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:36:49.838975Z",
     "start_time": "2022-09-06T09:36:49.824977Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3def17",
   "metadata": {},
   "source": [
    "The Court information is contained the \"court\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be56e3db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:36:51.910898Z",
     "start_time": "2022-09-06T09:36:51.892919Z"
    }
   },
   "outputs": [],
   "source": [
    "df['court'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ca7119",
   "metadata": {},
   "source": [
    "To extract the \"court_name\", we need to go into this dictionary and use the \"name\" key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18cd5a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:36:54.078471Z",
     "start_time": "2022-09-06T09:36:54.051466Z"
    },
    "id": "781fddfb-b064-4b57-9f7a-68774b246054"
   },
   "outputs": [],
   "source": [
    "df['court_name'] = df[\"court\"].apply(lambda x:x['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf745f3",
   "metadata": {},
   "source": [
    "We now need to conditionally select only those court names that are \"Delaware Court of Chancery\" and \"Delaware court of common pleas\" - and create separate dataframes out of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66a6e00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:37:50.197724Z",
     "start_time": "2022-09-06T09:37:50.174600Z"
    }
   },
   "outputs": [],
   "source": [
    "common_pleas = df[df['court_name'] == 'Delaware Court of Common Pleas'] \n",
    "common_pleas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681d8b72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:37:56.223218Z",
     "start_time": "2022-09-06T09:37:56.212224Z"
    }
   },
   "outputs": [],
   "source": [
    "len(common_pleas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1466af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:37:59.587925Z",
     "start_time": "2022-09-06T09:37:59.571917Z"
    },
    "id": "781fddfb-b064-4b57-9f7a-68774b246054"
   },
   "outputs": [],
   "source": [
    "chancery = df[df['court_name'] == 'Delaware Court of Chancery'] ## create a new dataframe called \"chancery\" which is a conditional subset\n",
    "chancery = chancery.sample(275) ## random sample of 275 chancery cases - since common pleas has only 275 cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05884b27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:38:04.304609Z",
     "start_time": "2022-09-06T09:38:04.291609Z"
    }
   },
   "outputs": [],
   "source": [
    "chancery.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557a3bb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-23T12:01:19.996472Z",
     "start_time": "2022-08-23T12:01:19.982473Z"
    }
   },
   "source": [
    "\n",
    "Now we can concatenate both of these dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128249a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:38:29.215683Z",
     "start_time": "2022-09-06T09:38:29.199654Z"
    },
    "id": "781fddfb-b064-4b57-9f7a-68774b246054"
   },
   "outputs": [],
   "source": [
    "concat = pd.concat([chancery, \n",
    "                    common_pleas]) ## create new dataframe called \"concat\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c995f2",
   "metadata": {},
   "source": [
    "Now that we have a dataframe with __only__ the Chancery and the Common please decisions, we can run the following cell which applies the **\"get_text\"** function that we created above, to the \"case_text\" column using pandas [apply function](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00720806",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:38:31.080492Z",
     "start_time": "2022-09-06T09:38:31.063494Z"
    },
    "id": "781fddfb-b064-4b57-9f7a-68774b246054"
   },
   "outputs": [],
   "source": [
    "concat['case_text'] = concat['casebody'].apply(lambda x: get_text(x)) ## use the function we created above called \"get_text\" \n",
    "                                                                      ## which will extract the text\n",
    "concat = concat[concat['case_text'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfc82d1",
   "metadata": {},
   "source": [
    "If you want to remove punctuation you can run the following cell which applies the \"remove_punctuation\" function to the \"case_text\" column, again, using lambda apply syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781fddfb-b064-4b57-9f7a-68774b246054",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:38:33.953194Z",
     "start_time": "2022-09-06T09:38:33.425387Z"
    },
    "id": "781fddfb-b064-4b57-9f7a-68774b246054"
   },
   "outputs": [],
   "source": [
    "concat['case_text'] = concat['case_text'].apply(lambda x:remove_punctuation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19649df2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:38:37.512963Z",
     "start_time": "2022-09-06T09:38:37.489960Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d1445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea0c1863",
   "metadata": {
    "id": "a6031e1c-e83f-495f-9415-1afeec5373ec"
   },
   "source": [
    "## Shifterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a9ece4",
   "metadata": {},
   "source": [
    "[Shifterator](https://shifterator.readthedocs.io/en/latest/cookbook/getting_started.html#case-study) is a pretty cool recent package which helps us intuitively understand and visualize the classical question of \"comparing texts\" - in the words of the creators \"word shift graphs are interpretable horizontal bar charts for visualizing how any two texts compare according to a given measure.\"  \n",
    "\n",
    "* Firstly, there are a number of different __measures__ that are discussed in the paper. From relative frequencies of words to \"Jensen-Shannon divergence\" - which is actually pretty commonly used in NLP. For the purposes of the class, it's not important to know the __math__ behind these measures - what's important is that some measures have certain benefits over others - and thus, visualizations will come out different. Essentially, all they do is show difference between texts. \n",
    "* The data input that it expects is a frequency count of words in a given text - and the `clean_text` function below does just that.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fd118f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:39:22.004853Z",
     "start_time": "2022-09-06T09:39:21.730856Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install shifterator\n",
    "import shifterator as sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f70756b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:40:03.626726Z",
     "start_time": "2022-09-06T09:40:03.617585Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "def clean_text(text_list):\n",
    "    temp = [remove_stopwords(individual_text.lower()) for individual_text in text_list]      # lowercase and  gensim's remove stopwords \n",
    "    temp = [individual_text.split() for individual_text in temp]                        # lowercase and tokenize the cases \n",
    "    temp = list(itertools.chain(*temp))\n",
    "    temp = collections.Counter(temp)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190f34bd",
   "metadata": {},
   "source": [
    "Let's now clean the text and create a counter object (frequency of words), which is required to work with shifterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6476f339",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:40:04.873269Z",
     "start_time": "2022-09-06T09:40:04.618251Z"
    },
    "id": "e6321949-8eb2-4f3d-b26f-d7d3459812f0"
   },
   "outputs": [],
   "source": [
    "clean_texts_chancery = clean_text(concat[concat['court_name'] == 'Delaware Court of Chancery']['case_text'].to_list())\n",
    "clean_texts_pleas = clean_text(concat[concat['court_name'] == 'Delaware Court of Common Pleas']['case_text'].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6799ae1",
   "metadata": {},
   "source": [
    "Now that we have our data in the appropriate format for the shifterator, we can try to examine the different measures shifterator provides. \n",
    "\n",
    "* The [shifterator cookbook](https://shifterator.readthedocs.io/en/latest/cookbook/index.html) goes over these in great detail. \n",
    "* Keep in mind that once we have the data, we don't need to make it again. \n",
    "* Also, even though I removed the stopwords, technically, this is not necessary as some measures (like the shannon entropy shift) can safely ignore stopwords. But I did that just in case.\n",
    "\n",
    "\n",
    "Let's start with the __proportion frequency shift__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776e0c34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:40:09.097341Z",
     "start_time": "2022-09-06T09:40:08.102350Z"
    }
   },
   "outputs": [],
   "source": [
    "proportion_shift = sh.ProportionShift(type2freq_1 = clean_texts_chancery,\n",
    "                                      type2freq_2 = clean_texts_pleas)\n",
    "proportion_shift .get_shift_graph(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b949d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:40:23.765984Z",
     "start_time": "2022-09-06T09:40:22.717963Z"
    },
    "id": "044e33e0-0566-483d-9b2b-ace1f66978a5"
   },
   "outputs": [],
   "source": [
    "entropy_shift = sh.EntropyShift(type2freq_1 = clean_texts_chancery, \n",
    "                                type2freq_2 = clean_texts_pleas, base=2)\n",
    "entropy_shift.get_shift_graph(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff554cf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:41:38.505946Z",
     "start_time": "2022-09-06T09:41:37.551937Z"
    }
   },
   "outputs": [],
   "source": [
    "jsd_shift = sh.JSDivergenceShift(type2freq_1 = clean_texts_chancery,\n",
    "                                 type2freq_2 = clean_texts_pleas,\n",
    "                                 weight_1=0.5,\n",
    "                                 weight_2=0.5,\n",
    "                                 base=2,\n",
    "                                 alpha=1)\n",
    "\n",
    "entropy_shift.get_shift_graph(); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdd1427",
   "metadata": {},
   "source": [
    "Do you see any differences between these measures? What words are emphasized?\n",
    "\n",
    "In order to answer this, you have to read the brief explanation that is provided in the [shifterator cookbook](https://shifterator.readthedocs.io/en/latest/cookbook/frequency_shifts.html#jensen-shannon-divergence-shifts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef3edcc",
   "metadata": {},
   "source": [
    "## Scattertext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a1e650",
   "metadata": {
    "id": "1f16c4ff-3f3a-4e0b-9b4c-5d662cbedc13"
   },
   "source": [
    "\n",
    "We are going to use [scattertext](https://github.com/JasonKessler/scattertext) to visualize text comparisons. You can take a case from Delaware court of Chancery vs Delaware Criminal court for example.\n",
    "\n",
    "Note: scattertext is more taxing than a simple measure like \"Jensen Shannon Divergence\" - so use it on small/medium corpuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc9cc7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:44:36.283079Z",
     "start_time": "2022-09-06T09:44:35.988071Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "16aec43b-6e8d-466a-9685-cb9279180401",
    "outputId": "f6d784c9-6ef3-4059-9d80-9a05e239a725"
   },
   "outputs": [],
   "source": [
    "#!pip install scattertext\n",
    "import scattertext as st"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8959ae8b",
   "metadata": {},
   "source": [
    "To work with scattertext we'll need two things:\n",
    "\n",
    "* A column that labels our data into two distinct classes - in this case `__court_name__` - because we care about the distinction between Chancery (1) vs Please (0)\n",
    "* Spacy's NLP model - which is why this might take a bit of a while "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd1b872",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:54:02.508188Z",
     "start_time": "2022-09-06T09:52:06.933680Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "corpus = st.CorpusFromPandas(concat,\n",
    "                             category_col='court_name',\n",
    "                             text_col='case_text',\n",
    "                             nlp = nlp).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51159aa8",
   "metadata": {},
   "source": [
    "Words associated with the Chancery Court"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96730e3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:54:44.659066Z",
     "start_time": "2022-09-06T09:54:43.473048Z"
    }
   },
   "outputs": [],
   "source": [
    "list(corpus.get_scaled_f_scores_vs_background().index[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4208aa82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:56:17.855203Z",
     "start_time": "2022-09-06T09:56:17.686231Z"
    }
   },
   "outputs": [],
   "source": [
    "term_freq_df = corpus.get_term_freq_df()\n",
    "term_freq_df['chancery score'] = corpus.get_scaled_f_scores('Delaware Court of Chancery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47f6a42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T09:56:29.936438Z",
     "start_time": "2022-09-06T09:56:29.887439Z"
    }
   },
   "outputs": [],
   "source": [
    "print(list(term_freq_df.sort_values(by='chancery score', ascending=False).index[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f191d2de",
   "metadata": {},
   "source": [
    "Import some libraries to display HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01282bb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T10:28:12.008545Z",
     "start_time": "2022-09-06T10:28:11.999547Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import IFrame\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b18f02",
   "metadata": {},
   "source": [
    "Produce HTML file - note that the parameters set here can determine how the visualization looks like - in particular \"minimum_term_frequency\" and \"pmi_threshold_coef\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb1e12a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T10:27:52.249250Z",
     "start_time": "2022-09-06T10:27:49.718252Z"
    }
   },
   "outputs": [],
   "source": [
    "html = st.produce_scattertext_explorer(corpus,\n",
    "          category='Delaware Court of Chancery',\n",
    "          category_name='Chancery',\n",
    "          not_category_name='Pleas',\n",
    "          minimum_term_frequency = 10, \n",
    "          pmi_threshold_coefficient = 8,\n",
    "          #transform=st.Scalers.dense_rank, \n",
    "          width_in_pixels=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e09d5dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T10:29:53.004266Z",
     "start_time": "2022-09-06T10:29:52.986267Z"
    }
   },
   "outputs": [],
   "source": [
    "file_name = 'chancery_vs_pleas.html'         \n",
    "with open(file_name, 'wb') as fn:\n",
    "    fn.write(html.encode('utf-8'))              \n",
    "\n",
    "display(IFrame(file_name, width=1000, height=650))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2fbded",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab2 : Final",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
