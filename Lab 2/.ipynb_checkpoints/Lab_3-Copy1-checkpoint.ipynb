{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIURErx442ZS"
   },
   "source": [
    "# Plotting and visualizations with DataFrames\n",
    "## Histograms\n",
    "Histograms are a nifty way to display quantitative information. The x-axis is typically a quantitative variable of interest, and the y-axis is generally a frequency. Plot a histogram of the losses, and then experiment with the bin sizes by uncommmenting out the italicized text (removing the # symbol)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T09:47:23.272060Z",
     "start_time": "2022-08-22T09:47:22.645060Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "oYVWEaZJBNfs",
    "outputId": "c9bace3a-0053-47a8-d274-82cd16cac7a7"
   },
   "outputs": [],
   "source": [
    "df.hist(\"median_house_value\", grid = False)\n",
    "\n",
    "# Try uncommenting the following lines and see the different results. What does this tell you about the parameters used?\n",
    "\n",
    "#df.hist(\"median_house_value\",bins = range(0,500000,1000) ,grid = False)\n",
    "#df.hist(\"median_house_value\" ,grid = False)\n",
    "#df.hist(\"median_house_value\",grid = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UuESGb55Bg_d"
   },
   "source": [
    "## Scatter Plots\n",
    "Scatter plots are generally used to relate two variables to one another. They can be useful when trying to infer relationships between variables, visualize simple regressions, and get a general sense of the \"spread\" of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T09:47:33.615206Z",
     "start_time": "2022-08-22T09:47:33.435202Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "7z3S3AyPBeaY",
    "outputId": "68469ae7-a6fa-4236-9d49-c9969985ebb7"
   },
   "outputs": [],
   "source": [
    "# Median house value vs Median income. Do you spot a correlation?\n",
    "df.plot.scatter(x = \"median_income\", \n",
    "                y = \"median_house_value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-VUiGIw9IF8"
   },
   "source": [
    "# **Part 2 - Introduction to API**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For LS190 , we'll be extensively working with the **[case.law](https://case.law/)** database - which is a database of **360 years of United States caselaw.** To access this data we'll need to develop a simple understanding of APIs.\n",
    "\n",
    "* API is the acronym for **Application Programming Interface** - a pretty vague acronym, if you ask me (unless you are a CS person who can explain what this means). The term itself most likely comes from the early days of computing. \n",
    "\n",
    "* For the purposes of this class, the important thing to note is that an **API allows us to interact with, access and download data from the case.law database.** This is why a **[case.law API KEY](https://case.law/docs/site_features/api)** becomes important - as this key allows you to download the case.law data. You can think of an API key as a magical phrase like \"Open Sesame\" - which lets you access a database where hidden treasures of data await.\n",
    "\n",
    "* The overwhelming amounts of data has made **APIs and API KEYS** the standard means of accessing data. Different organizations which store data have different means of accessing it. For example, there's the **[Twitter API](https://developer.twitter.com/en/docs/twitter-api)** if you want to study tweets. Or **[NYTimes API](https://developer.nytimes.com/apis)** if you want to study the New York Times Archive.\n",
    "\n",
    "The API for case.law is very-well documented and you can find examples of how to use the API by following the various **[jupyter notebooks they provided](https://github.com/harvard-lil/cap-examples)**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T10:11:28.973070Z",
     "start_time": "2022-08-22T10:11:28.853047Z"
    },
    "id": "Nmt87jZXgsuj"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import lzma\n",
    "import json\n",
    "\n",
    "from config import settings_base as settings \n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we are importing a couple of libraries. \n",
    "* **`lzma`** allows us to decompress the case.law data\n",
    "* **`json`** allows us to access the *dictionary* data structure \n",
    "* **`config`** is a folder which contains **settings_base** python script. This script should contain your **API KEY** \n",
    " * Note that each API key is unique and different. Because these notebooks are published on github, we cannot share the API Key. This is why I ask you to get the API key from the kind folks at case.law as soon as possible.\n",
    "* Finally, **`utils`** is a python script which contains helper functions written by case.law folks which allow us to download their data.\n",
    "\n",
    "The examples code below is based on the **[example notebooks](https://github.com/harvard-lil/cap-examples)**  example notebooks  written by the wonderful people working at case.law. The case.law project is incredibly important as it allows us to access **huge amounts of case text data** without having to pay subscription for services like LexisNexis or Westlaw. This is a wonderful example of data democratization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T10:11:58.251704Z",
     "start_time": "2022-08-22T10:11:32.101464Z"
    },
    "id": "fATB2Hdhsa3n"
   },
   "outputs": [],
   "source": [
    "# Get Case Data for Hawaii (as it's a small-ish jurisdiction)\n",
    "compressed_file = utils.get_and_extract_from_bulk(jurisdiction=\"Hawaii\", \n",
    "                                                  data_format=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T10:22:37.246311Z",
     "start_time": "2022-08-22T10:22:37.242326Z"
    },
    "id": "HNuUBD8DXBnX"
   },
   "outputs": [],
   "source": [
    "# Assume we are dealing with json data (if data_format is changed to xml or\n",
    "# change this cell's os.path.join line)\n",
    "if not compressed_file.endswith('.xz'):\n",
    "  compressed_file = os.path.join(compressed_file, \"data\", \"data.jsonl.xz\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T10:23:23.262527Z",
     "start_time": "2022-08-22T10:22:41.786573Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q0NIKWEPsfIF",
    "outputId": "75b9a2e1-29cc-471e-aec2-170969a0b1bf"
   },
   "outputs": [],
   "source": [
    "cases = []\n",
    "print(\"File path:\", compressed_file)\n",
    "with lzma.open(compressed_file) as infile:\n",
    "    for line in infile:\n",
    "        record = json.loads(str(line, 'utf-8'))\n",
    "        cases.append(record)\n",
    "\n",
    "print(\"Case count: %s\" % len(cases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T10:23:23.961528Z",
     "start_time": "2022-08-22T10:23:23.265510Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704
    },
    "id": "smiRy_M3siNq",
    "outputId": "877effb5-053c-404c-aed9-be2f61f2a7a4"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(cases)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have access to case.law data, Hawaii dataset. Fascinating!\n",
    "\n",
    "You can explore the data further here if you'd like. Note that the actual **text** is contained within a dictionary in the column named **casebody.** We will be exploring how to access the text of decisions below when we will be introducing **concordances**\n",
    "\n",
    "Note:\n",
    "I also, again, encourage you to check out the case.law **[example notebooks](https://github.com/harvard-lil/cap-examples)**. For instance, the **[Cartwright notebook - which shows who was Illinois' most prolific judge](https://github.com/harvard-lil/cap-examples/blob/develop/bulk_exploration/cartwright.ipynb)** is pretty fascinating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOnadrCz_4SR"
   },
   "source": [
    "# **Part 3 - Introduction to some simple NLP Tasks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1kwIIDaB7IG"
   },
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRAst2E6ADhm"
   },
   "source": [
    "**[spacy](https://spacy.io/usage/spacy-101#features)** is an open-source NLP library which we'll be using to analyze features about textual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T10:29:05.882754Z",
     "start_time": "2022-08-22T10:28:53.233054Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dabEbEUZAGjS",
    "outputId": "0c2ba945-4efc-4e92-f46d-209b98362060"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "text = \"Today, I had a great time visiting Disneyland!\"\n",
    "\n",
    "# Configures pipeline for language processing - \n",
    "# we're basically loading a model called \"en_core_web_sm\" - a trained model for English language\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Creates a pipeline for the string associated with variable \"text\"\n",
    "pipe = nlp(text)\n",
    "\n",
    "for token in pipe:\n",
    "  print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iIkUmNNAMk3"
   },
   "source": [
    "Python's `split()` function is similar, but you'll notice that it's not so easy to separate words from punctuation, as compared to spaCy's tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T10:29:57.065111Z",
     "start_time": "2022-08-22T10:29:57.047111Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q1VL0dMWAZcR",
    "outputId": "dbdfe01c-7a72-4c9a-8d58-4d33dabc32bd"
   },
   "outputs": [],
   "source": [
    "# splitting on whitespace\n",
    "text.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziPDOipMAeb7"
   },
   "source": [
    "**spaCy** is great because it's easy to retrieve various elements about the tokens within your text. We run a model that's already available - namely \"en_core_web_sm\"\n",
    "\n",
    "For example, say with every token, you wanted to know the **part-of-speech** - is it a Verb or a Noun?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T10:31:16.554854Z",
     "start_time": "2022-08-22T10:31:16.546852Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4JEIH21uAg7i",
    "outputId": "e8062bf0-692d-4bd0-b97f-7e1fcc980abe"
   },
   "outputs": [],
   "source": [
    "token_pos_pairs = []\n",
    "\n",
    "for token in pipe:\n",
    "  token_pos_pairs.append([token, token.pos_])\n",
    "\n",
    "token_pos_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_UXi7mPAojB"
   },
   "source": [
    "The \"lemma\" of a word can be considered its base form. For example, *eating*, *ate*, and *eat* all have the same lemma: *eat*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tR4VXdGnA6mK"
   },
   "source": [
    "**Question 5: Using the for loop above as inspiration, create a list `token_lemma_pairs`. This list will be the same shape as token_pos_pairs, but contain the lemma rather than part-of-speech.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8TK2yYfaBK1O",
    "outputId": "74764278-5e53-4a1b-848f-e17c84a8d899"
   },
   "outputs": [],
   "source": [
    "token_lemma_pairs = []\n",
    "\n",
    "for token in pipe:\n",
    "  ...\n",
    "\n",
    "# The shape of token_lemma_pairs should match token_pos_pairs in the last coding cell\n",
    "token_lemma_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bE78hnr3Bdjb"
   },
   "source": [
    "Let's say now, we want to create a dictionary with the keys being terms within a text, and values being the number of times the terms are present. In this case, tokenization with spacy can help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iYjWZkjFBnnv",
    "outputId": "37c7c985-488e-4fb9-e6e2-15a1829b93ee"
   },
   "outputs": [],
   "source": [
    "text = \"Hey, what'd you think about the new movie?\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "pipe = nlp(text).to_array('LANG')\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_uKr1zYEBp1s",
    "outputId": "79525ede-e8f2-498a-b7c9-d4615c90b64b"
   },
   "outputs": [],
   "source": [
    "# Replace the 4 ellipses in the for-loop\n",
    "\n",
    "# word_counts is a dictionary with tokens as the keys and counts as the values\n",
    "word_counts = {}\n",
    "\n",
    "# Creates a pipeline of tokens for our text\n",
    "text = \"hi hi hi hi hi hey hello hey\" #REPLACE WITH A FILE!!!! This is just a test\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "pipe = nlp(text)\n",
    "\n",
    "tokens = []\n",
    "\n",
    "for token in pipe:\n",
    "  tokens.append(str(token))\n",
    "\n",
    "# word_counts[KEY] accesses a VALUE (count) associated with a KEY (token/word)\n",
    "for token in tokens:\n",
    "  # If the word already exists in the dictionary, what should you do to its count? \n",
    "  if token in word_counts:\n",
    "    word_counts[token] += 1\n",
    "  # What if it doesn't exist?\n",
    "  else:\n",
    "    word_counts[token] = 1\n",
    "\n",
    "word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsYkmeMsCBFy"
   },
   "source": [
    "# REGEX - Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ei3cPafKCCms"
   },
   "source": [
    "**Sets, Quantifiers, and Special Characters**\n",
    "\n",
    "Regex (regular expressions) is a very powerful tool to find patterns in text. One of the best ways to learn Regex is by using Regex 101 to practice matching words in a body of text.\n",
    "\n",
    "[Regex101](https://regex101.com/)\n",
    "\n",
    "[Regex Reference Sheet](http://www.rexegg.com/regex-quickstart.html#ref)\n",
    "\n",
    "For example, say we had a text and we wanted to find every instance of a word within that text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zBeFZsbJCJqK",
    "outputId": "42473dd9-c5d7-4043-a836-6bee70f9caaf"
   },
   "outputs": [],
   "source": [
    "# Just run this cell\n",
    "import regex as re\n",
    "text = \"Samuel and I went down to the river yesterday! Samuel isn't a very good swimmer, though. Good thing our friend Sahit was there to help.\"\n",
    "\n",
    "# the findall() function finds every instance of a specified word pattern within a text\n",
    "re.findall(r'Samuel', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3RTPiIhCOi5"
   },
   "source": [
    "Let's say that instead of only wanting to find Samuel, we wanted to find every word in the text starting with 'Sa'. What would we do? Use pattern matching!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kndfT3OeCNh9",
    "outputId": "312fffa7-d93f-486f-f7e5-d43e7595fd97"
   },
   "outputs": [],
   "source": [
    "re.findall(r'Sa[a-z]*', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLgX7o7ICU3p"
   },
   "source": [
    "You may be wondering what the [a-z] in the Sa[a-z] pattern means. This is called a **set** in regex. When characters are within a set, such as  [abcde], any one character will match. However, regex has a special rule where [a-z] means the same thing as [abcde...xyz].\n",
    "\n",
    "Here are some more:\n",
    "~~~ \n",
    "[0-9]        any numeric character\n",
    "[a-z]        any lowercase alphabetic character\n",
    "[A-Z]        any uppercase alphabetic character\n",
    "[aeiou]      any vowel (i.e. any character within the brackets)\n",
    "[0-9a-z]     to combine sets, list them one after another \n",
    "[^...]       exclude specific characters\n",
    "~~~\n",
    "\n",
    "\n",
    "You still may be wondering how the entirety of Sahit was able to be matched if only one character within [a-z] would match. The answer is something called a **quantifier**!\n",
    "\n",
    "Rules:\n",
    "~~~ \n",
    "*        0 or more of the preceding character/expression\n",
    "+        1 or more of the preceding character/expression\n",
    "?        0 or 1 of the preceding character/expression\n",
    "{n}      n copies of the preceding character/expression \n",
    "{n,m}    n to m copies of the preceding character/expression \n",
    "~~~\n",
    "\n",
    "Say that now, you only wanted to return Samuel when the name was mentioned at the beginning of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8BN1ZQwqCV7J",
    "outputId": "7f38c366-58e2-440b-85f3-9c53496fc486"
   },
   "outputs": [],
   "source": [
    "re.findall(r'^Samuel', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZMLtpFaCa-4"
   },
   "source": [
    "**Special characters**, such as the *^* which was just used in the pattern above, match strings that have a specific placement in a sentence. For example, *^* matches the subsequent pattern only if it is at the beginning of the string. This is why only a single 'Samuel' was returned.\n",
    "\n",
    "Rules:\n",
    "~~~ \n",
    ".         any single character except newline character\n",
    "^         start of string\n",
    "$         end of entire string\n",
    "\\n        new line\n",
    "\\r        carriage return\n",
    "\\t        tab\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ojtbv71XCojp"
   },
   "source": [
    "**Python RegEx Methods**\n",
    "\n",
    "* `re.findall(pattern, string)`: Returns all phrases that match your pattern in the string.\n",
    "\n",
    "* `re.sub(pattern, replacement, string)`: Return the string after replacing the leftmost non-overlapping occurrences of the pattern in string with replacement\n",
    "\n",
    "* `re.split(pattern, string)`: Split string by the occurrences of pattern. If capturing parentheses are used in pattern, then the text of all groups in the pattern are also returned as part of the resulting list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYqg1kc1Iv8j"
   },
   "source": [
    "# **Part 4 - Concordances and Collocation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igkvUNygJVSq"
   },
   "source": [
    "## Concordances\n",
    "A concordance view shows us every occurrence of a given word, together with some context. Concordances are fundamentally important if we want to understand the meaning of a word in a context.\n",
    "\n",
    "Here we look up the word \"petition\" in casebody of the California Dataset by entering text followed by a period, then the term concordance, and then placing “petition” in parentheses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T10:42:00.999872Z",
     "start_time": "2022-08-22T10:42:00.987882Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qq7Oag4tJGaU",
    "outputId": "e79058b0-2b2f-4ec2-c858-6b384e1cd6a8"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.text import Text\n",
    "from nltk.tokenize import word_tokenize # import tokenizer function from nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we created a dataframe called **df** which consists of downloaded cases from Hawaii. \n",
    "We will now try to access the text of those decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T10:41:11.112886Z",
     "start_time": "2022-08-22T10:41:11.092599Z"
    }
   },
   "outputs": [],
   "source": [
    "case_body = df['casebody'][0]['data']['opinions'][0]['text'] # this function looks into casebody column\n",
    "                                                             # first elememnt in it, aka [0], then 'data', then 'opinions'\n",
    "                                                             # then the first element again [0], then text\n",
    "                                                             # basically - it's complex data structure - \n",
    "                                                             # a dictionary with a list with a dicitonary with a list!\n",
    "case_body[:10000] ## print the first 10000 characters of this text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T10:42:08.549641Z",
     "start_time": "2022-08-22T10:42:08.485616Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JM0A4YP_JKj_",
    "outputId": "1c41ff03-265c-4b71-b2f9-c10fb483a23a"
   },
   "outputs": [],
   "source": [
    "case_body_tokenized = word_tokenize(case_body)\n",
    "text = Text(case_body_tokenized)\n",
    "text.concordance(\"petition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEAq9YRTjFHH"
   },
   "source": [
    "**Question 6: write your own code to explore the occurrence of the word *court***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HtPjYeCwjGWQ",
    "outputId": "10d17730-2e86-498d-fcbb-62268d4f4555"
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTR1I6NpaxZt"
   },
   "source": [
    "## Collocation\n",
    "Collocations are expressions of multiple words which commonly co-occur. For example, the top ten bigram collocations in casebody of the downloaded case.law dataset are listed below, as measured using **Pointwise Mutual Information**. \n",
    "\n",
    "[A pretty good explanation of PMI](https://stats.stackexchange.com/a/522504) is given on **stackexchange.**\n",
    "\n",
    "Note: stackexchange (and google generally) is a wonderful resource for all things relating to NLP and statistics. Although for this course, I do not emphasize concrete statistical knowledge or mathematical formulas, you should still get some **intuitive** understanding of what these measures like PMI do. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T10:46:36.232201Z",
     "start_time": "2022-08-22T10:46:36.228226Z"
    },
    "id": "AWJNDIOIJNED"
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "fourgram_measures = nltk.collocations.QuadgramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T10:46:43.125438Z",
     "start_time": "2022-08-22T10:46:43.085474Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dSsFAHq2JPtl",
    "outputId": "78ee1dac-e3bc-4675-df13-2179bce20cfb"
   },
   "outputs": [],
   "source": [
    "bigram_finder = BigramCollocationFinder.from_words(case_body_tokenized)\n",
    "bigram_finder.nbest(bigram_measures.pmi, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see any interesting patterns that emerge from the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pu7RXh6pjW_t"
   },
   "source": [
    "**Question 7: Using the methods defined above, find the top 10 trigram and fourgram collocations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lk2b-A0qJSRh"
   },
   "outputs": [],
   "source": [
    "# Trigram\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aq0OZCEHDnCH"
   },
   "outputs": [],
   "source": [
    "# FourGram\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fND5UzC7C_SH"
   },
   "source": [
    "Congratulations! You have finished Lab 1!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab1 : Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
