{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "914af3ee",
   "metadata": {},
   "source": [
    "# Lab 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3172ce5e",
   "metadata": {},
   "source": [
    "This lab will introduce \"word embeddings\" (or a mapping of a word into a vector space), beginning with the word2vec algorithm - which started a revolution in NLP in 2013. This revolution is still ongoing, and it is currently culminating with \"contextualized word embeddings\" and \"large language models\" which even outperform humans at certain tasks (GPT3, BERT) - but conceptually BERT is a model that's built on the same concept as word2vec - ie that words can be distributed in a vector space - but is more \"dynamic\" than word2vec - ie can capture context information. We will cover this in next week's lab.\n",
    "\n",
    "\n",
    "So far we have focused on bag-of-word approaches i.e representations of documents and words as a vectors **documents** and **word frequencies** (if we want TFIDF matrix, we apply TFIDF weighting). The problem with bag of word approaches is that they do not capture any information about similarity or meaning of words. \n",
    "\n",
    "Recall the example Document-Term Matrices that we used [here](https://i.stack.imgur.com/hMe5D.png)\n",
    "or\n",
    "[here](https://i.stack.imgur.com/Aj2A7.png). The bag of words representation has no information of context - the columns are even alphabetic - hence \"bag of words.\"\n",
    "\n",
    "\n",
    "## Conceptual point - words and documents can be thought of as vectors in the DTM\n",
    "As we should be familiar by now, in a Document Term Matrix (DTM) rows represent documents, and the columns words. \n",
    "\n",
    "Note that the words (columns) are **sparse vectors** - meaning, that for most words, the column is going to be full of 0s - reflecting the fact that not every word is used in every document. Also note that the DTM lists words alphabetically, reflecting the fact that order is completely disregarded - and thus, context is ignored entirely. \n",
    "\n",
    "\n",
    "## Words as vectors \n",
    "If we were to take the **word1** as a vector, we would have **[0,0,0,1]** vector. And if we represent another **word2** as a vector we'd get **[0,1,0,0]**. Cosine similarity between two orthogonal vectors is 0 - they point in completely different directions. Thus, even if we wanted to get \"similarity\" between two words in a DTM, we wouldn't get much information. But we still have some luck using consime similarity with documents. \n",
    "\n",
    "## Documents as vectors \n",
    "Document comparison using cosine similarity is another fruitful task. \n",
    "\n",
    "\n",
    "## Word2vec and [distributional hypothesis](https://en.wikipedia.org/wiki/Distributional_semantics)\n",
    "\n",
    "As we will see in a bit, word2vec allows us to represent words as __dense__ vectors. Each word is __embedded__ in a vector space of a fixed dimension (usually 300) where __similar words__ are located together in a vector space. This allows us to do similarity calculation between words - thus gaining insight into their semantic content. The fact that each word is embedded as a vector in a vector space is why each word represented by this method is called a __word embedding__\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1N-fteDEngtx",
   "metadata": {
    "id": "1N-fteDEngtx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import lzma\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from config import settings_base as settings\n",
    "from config import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5924eb9b",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GACSTLWdnghu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GACSTLWdnghu",
    "outputId": "cd09337a-0efd-424b-f5f5-a26418972bb8"
   },
   "outputs": [],
   "source": [
    "compressed_file = utils.get_cases_from_bulk(jurisdiction=\"Delaware\", data_format=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gGz7gn-Ont9M",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gGz7gn-Ont9M",
    "outputId": "7ade51d2-6990-463e-b964-1591b21f872a"
   },
   "outputs": [],
   "source": [
    "cases = []\n",
    "print(\"File path:\", compressed_file)\n",
    "with lzma.open(compressed_file) as infile:\n",
    "    for line in infile:\n",
    "        record = json.loads(str(line, 'utf-8'))\n",
    "        cases.append(record)\n",
    "\n",
    "print(\"Case count: %s\" % len(cases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iWA2tMOQnuV1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 687
    },
    "id": "iWA2tMOQnuV1",
    "outputId": "9854875c-5fb0-4622-a98c-6772f2f86b2d"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(cases)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c367d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_data = []\n",
    "for case in cases:\n",
    "    for opinion in case[\"casebody\"][\"data\"][\"opinions\"]:\n",
    "        temp = {}\n",
    "        keys = list(case.keys())\n",
    "        keys.remove('casebody')\n",
    "        for key in keys:         \n",
    "            temp[key] = case[key]\n",
    "        keys = list(opinion.keys())\n",
    "        for key in keys:         \n",
    "            temp[key] = opinion[key]\n",
    "        opinion_data.append(temp)\n",
    "        \n",
    "df = pd.DataFrame(opinion_data)\n",
    "df[\"citations\"] = df[\"citations\"].apply(lambda x:x[0]['cite'])\n",
    "df[\"court\"] = df[\"court\"].apply(lambda x:x['name'])\n",
    "df[\"decision_date\"] = df[\"decision_date\"].apply(lambda x:int(x[:4]))\n",
    "\n",
    "#df = df[df['court'] =='Delaware Court of Chancery'] ## if we want to just focus on one court\n",
    "\n",
    "df[\"text\"] = df[\"text\"].str.lower()\n",
    "df = df.drop([\"docket_number\", \"first_page\", \n",
    "                                \"last_page\", \"name\",\n",
    "                                \"reporter\", \"volume\", \"jurisdiction\"], axis=1)\n",
    "df = df[[\"name_abbreviation\", \"decision_date\", \"court\", \"author\", \"type\", \"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fa0cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5pUeW8MMrV7F",
   "metadata": {
    "id": "5pUeW8MMrV7F"
   },
   "outputs": [],
   "source": [
    "sample_df = df.sample(500, \n",
    "                      replace=False , \n",
    "                      random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca225eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402127ff",
   "metadata": {},
   "source": [
    "## Part 1 - Cosine similarity for document similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505106c9",
   "metadata": {},
   "source": [
    "According to Wikpiedia, \"Cosine similarity is a measure of similarity between two sequences of numbers.\" \n",
    "\n",
    "[In essence](https://storage.googleapis.com/lds-media/images/cosine-similarity-vectors.original.jpg) - if there is a small angle between two vectors (which is just a sequence of numbers) - that means they are similar. \n",
    "\n",
    "Don't underestimate the usefulness of cosine similarity measures. Imagine you have to find the most similar case to another case in a corpus. How would you go about it? Cosine similarity can help here.\n",
    "\n",
    "The idea is very simple:\n",
    "\n",
    "* Cosine(10 degrees) gives you 0.98 __\"cosine similarity\"__ measure. This can be interpreted as vectors are \"98% similar\". \n",
    "\n",
    "* Cosine(90 degrees) gives you a 0 __\"cosine similiarty\"__. This can be interpreted as vectors are \"0% similar\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dbf3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tf_vectorizer = CountVectorizer(min_df=0.1,\n",
    "                         max_df=.9,  \n",
    "                         max_features=1000,\n",
    "                         stop_words='english',\n",
    "                         ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aec576",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tf = tf_vectorizer.fit_transform(sample_df['text'])\n",
    "\n",
    "tf = pd.DataFrame(data = X_tf.toarray(), \n",
    "                  columns = tf_vectorizer.get_feature_names())\n",
    "\n",
    "tf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8866799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=0.1,\n",
    "                                   max_df=.9,  \n",
    "                                   max_features=1000,\n",
    "                                   stop_words='english',\n",
    "                                   ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfc9bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf = tfidf_vectorizer.fit_transform(sample_df['text'])\n",
    "\n",
    "tf_idf = pd.DataFrame(data = X_tfidf.toarray(), \n",
    "                      columns = tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "tf_idf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29f833e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KtYGwdw9OwJt",
    "outputId": "c99788fd-2a55-4e26-990d-9786b6d6df28"
   },
   "outputs": [],
   "source": [
    "# the cosine similarity measures similarity between rows of a matrix - making it into a Square matrix.\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KtYGwdw9OwJt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KtYGwdw9OwJt",
    "outputId": "c99788fd-2a55-4e26-990d-9786b6d6df28"
   },
   "outputs": [],
   "source": [
    "cos_sim_tf = cosine_similarity(X_tf)\n",
    "cos_sim_tfidf = cosine_similarity(X_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec715ca5",
   "metadata": {},
   "source": [
    "### CountVectorizer cosine similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ab5654",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_cos_sim = pd.DataFrame(data = cos_sim_tf, \n",
    "                          columns = sample_df['name_abbreviation'],\n",
    "                          index = sample_df['name_abbreviation'])\n",
    "\n",
    "cv_cos_sim.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ed0449",
   "metadata": {},
   "source": [
    "We can sort the column values to get the \"top similar\" cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8e2540",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_cos_sim.sort_values(by='Silvers v. Jones', \n",
    "                          ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bd4f6e",
   "metadata": {},
   "source": [
    "### TFIDF cosine similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503c1880",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_cos_sim = pd.DataFrame(data = cos_sim_tfidf, \n",
    "                             columns = sample_df['name_abbreviation'],\n",
    "                             index = sample_df['name_abbreviation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39abe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_cos_sim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c788125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_cos_sim.sort_values(by='Silvers v. Jones', \n",
    "                          ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510c5fb6",
   "metadata": {},
   "source": [
    "Notice, there's a bit of a difference between TF and TFIDF similarity scores. \n",
    "\n",
    "For instance, 3rd most similar to __Silvers v. Jones__ in TFIDF matrix is __Hayes v. Hayes__ and not __National Building, Loan & Provident Ass'n v. Alfree.__\n",
    "\n",
    "Let's actually read those \"similar cases\" to Silvers v Jones - are they actually similar? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad183f08",
   "metadata": {},
   "source": [
    "### Finding similar case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d44b792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(case):\n",
    "    return sample_df.name_abbreviation[sample_df.name_abbreviation == case].index.tolist()[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f242edef",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_of_interest = 'Vredenburgh v. Jones'\n",
    "\n",
    "get_index(case_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c02d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_df['text'][get_index(case_of_interest)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86632d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Silvers v. Jones is 4528\n",
    "sample_df['text'][4528]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5O3PPiPIPKmv",
   "metadata": {
    "id": "5O3PPiPIPKmv"
   },
   "source": [
    "They both seem to be talking about legacies, estates and other things related to inheritance laws.\n",
    "\n",
    "Seems like cosine similarity is working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0a0e69",
   "metadata": {},
   "source": [
    "__NOTE:__  Recall that we could also represent documents as \"topics\" rather than words - this can also be used for cosine similarity purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcbf09d",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6851eca1",
   "metadata": {},
   "source": [
    "## Part 2 - word2vec: some theory\n",
    "Now that we know what cosine similarity is  we can move on to Word2Vec. \n",
    "\n",
    "The theory behind the word2vec algorithm relies on two theoretical foundations - \n",
    "\n",
    "* **[Distributional Semantics](https://en.wikipedia.org/wiki/Distributional_semantics),** - ie that \"meaning\" of words is known by the context in which the word is used, and \n",
    "\n",
    "* **[Language Modeling](https://thegradient.pub/content/images/2019/10/lm-1.png)** - a task in NLP where you predict the next word in a sequence based on probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e87990",
   "metadata": {},
   "source": [
    "###  1) Distributional Semantics \n",
    "\n",
    "Word2vec algorithm is based on a Linguistic theory called \"distributional semantics\". \n",
    "\n",
    "This theory can be summerized by the linguist Firth's famous statement that **You shall know a word by the company it keeps**. This should not be surprising to anyone who encountered a weird word in a book - we usually tend to re-read the sentence and look for other words around the word we don't know. __Thus, we kinda get an idea of what the word is by looking at other words around it.__ \n",
    "\n",
    "* For example - we all know Lewis Carroll's famous poem [Jobberwocky](https://www.poetryfoundation.org/poems/42916/jabberwocky)\n",
    "\n",
    "Distributional semantics assumes a **distributional hypothesis**. In simple terms, distributional hypothesis argues that the usage of words is **a distribution.** Not only that, but the distribution is constrained/changes depending on various contexts. What this means is that there's only a __limited number of words that can occur in a given context.__\n",
    "\n",
    "#### Consider the following examples (recall, that we did something similar in the kindgergarden - this is how fundamental this stuff is):\n",
    "\n",
    "I like to think of this as follows: consider these example: \n",
    "* 1) **You are a ___________** \n",
    "\n",
    "(how many words can fit in the blank here? ie what is the **\"distribution\"**, probability wise, what words can fit here?) \n",
    "\n",
    "* 2) **You are a very very unlikable _____** \n",
    "\n",
    "(could we say that less words can in the blank than before) \n",
    "\n",
    "* 3) **I am typing on a _____** \n",
    "\n",
    "(how many words can fit in the blank space? - probably only a couple - a typewriter/computer/my phone). The distribution is more constrained. It definitely has to be a noun.)\n",
    "\n",
    "* 4) **I like drinking ____** \n",
    "\n",
    "(how many words can fit in the blank space - probably a billion) \n",
    "\n",
    "* 5) **I like drinking  ____, ice cold** \n",
    "\n",
    "(less words can fit here - for example, we can't talk about coffee any more, unless you drink ice cold coffee) \n",
    "\n",
    "* Thus, words that co-occur in the same context have similar meanings/functions/usages (you can't drink a \"table\" for example - your language is constrained and the context determines the word that you will use)\n",
    "\n",
    "#### This \"fill in the blank exercise\" above, when done by computers is called a [\"language modeling task.\"](https://miro.medium.com/max/1400/1*_MrDp6w3Xc-yLuCTbco0xw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73f041f",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "###  2) Language modeling with word2vec\n",
    "\n",
    "This \"fill in the blanks\" exercise that we did in kindergarden is unironically the way language modeling works. \n",
    "\n",
    "\n",
    "There are two model architectures for word2vec:\n",
    "\n",
    "* The first of the architectures is called __\"continious bag of words\"__ (CBOW) which predicts the **current word based on the context** (ie a word is blanked out and the algorithm looks at the data to see which words fit best/highest probability), \n",
    "\n",
    "* The second architecture, __\"Skip-gram\"__ (SGNS) predicts __surrounding words given the current word__ (it's literally  called **\"skipgram\"** ie - you \"skip\" an \"ngram\"). \n",
    "\n",
    "In both CBOW and SGNS You set the window size (context size around a target word) and the algorithm does this for all the words. \n",
    "\n",
    "See page 5 of the famous word2vec paper [here](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "\n",
    "\n",
    "Recall that theoretically speaking when we learn word vector representations via context information, we kinda do **the same thing as a concordance,** only on a much larger scale. That's pretty cool! - one can say that word2vec is actually fundamentally based on concordances.\n",
    "\n",
    "The math is not important here. Conceptually, word2vec algorithm works via a prediction task where you use as input the context word vectors (from the DTM) and for output, you know what the word is - and you use this to update the values in the __\"hidden layer\"__ - which subsequently becomes your \"word embedding\" - ie a dense representation of a word (rather than sparse). The vectors __keep updating__ until the algorithm gets matching predictions for the output word from the context words (in the case of CBOW). \n",
    "\n",
    "\n",
    "\n",
    "When trained on very large corpora (like all of English Wikipedia) it can perform very strong analogies such as finding that the vector corresponding the most to the output of the operation ['king' - 'man' + 'woman' is 'queen'](https://static.packt-cdn.com/products/9781787287600/graphics/d4b8d439-e136-44f7-895d-71de1d84342c.png)\n",
    "\n",
    "[Word2vec is very powerful!](https://www.distilled.net/uploads/word2vec_chart.jpg) - this is precisely the \"analogical reasoning\" that we humans are so good at. And remember - __analogy is one of the key tasks of a judge__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48ae212",
   "metadata": {
    "id": "f48ae212"
   },
   "source": [
    "## Part 3 - Word2Vec with gensim library\n",
    "\n",
    "For this part, we'll be using the [Gensim library](https://radimrehurek.com/gensim/auto_examples/core/run_corpora_and_vector_spaces.html#sphx-glr-auto-examples-core-run-corpora-and-vector-spaces-py) to make our very own word2vec model\n",
    "\n",
    "Note that word2vec requires sentences as inputs.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68317ef7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68317ef7",
    "outputId": "db2b6d9f-40d1-4150-dbe4-f5613d37e3b4"
   },
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502d5378",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lr93HUkdMJ_C",
    "outputId": "414d31b3-7097-4709-fa3b-62561ed600b1"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stoplist = set(stopwords.words('english'))\n",
    "\n",
    "def normalize_text(doc):\n",
    "    doc = doc.replace('\\r', ' ').replace('\\n', ' ')\n",
    "    lower = doc.lower() # all lower case\n",
    "    nopunc = lower.translate(str.maketrans('', '', string.punctuation)) # remove punctuation using translate\n",
    "    words = nopunc.split() # split into tokens\n",
    "    nostop = [w for w in words if w not in stoplist] # remove stopwords\n",
    "    no_numbers = [w if not w.isdigit() else '#' for w in nostop] # normalize numbers\n",
    "    return no_numbers\n",
    "\n",
    "def get_sentences(doc):\n",
    "    sent = []\n",
    "    for raw_text in sent_tokenize(doc):\n",
    "        normalized = normalize_text(raw_text)\n",
    "        sent.append(normalized)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80667470",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lr93HUkdMJ_C",
    "outputId": "414d31b3-7097-4709-fa3b-62561ed600b1"
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for doc in sample_df['text']:\n",
    "    sentences += get_sentences(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feba4816",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce7a70a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lr93HUkdMJ_C",
    "outputId": "414d31b3-7097-4709-fa3b-62561ed600b1"
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_model = Word2Vec(sentences,  # list of tokenized sentences\n",
    "               workers = 4, # Number of threads to run in parallel\n",
    "               vector_size=100,  # Word vector dimensionality     \n",
    "               min_count = 2, # Minimum word count  \n",
    "               window = 10 # Context window size      \n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5-pY8zPLNjAh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-pY8zPLNjAh",
    "outputId": "8428c7aa-ac72-401d-f433-59cb4401aa23"
   },
   "outputs": [],
   "source": [
    "words = list(w2v_model.wv.index_to_key)\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b05d21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97b05d21",
    "outputId": "5661774e-5f67-4635-afa8-401c3020737e"
   },
   "outputs": [],
   "source": [
    "## how many words in vocab\n",
    "print(len(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81250969",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ef4cfa64",
    "outputId": "c15b3b98-70c9-4409-9e01-2b6b26d9641d"
   },
   "outputs": [],
   "source": [
    "## Print actual values of word embedding - this is the hidden leayer aka the word embedding we \"learned\"\n",
    "\n",
    "print(w2v_model.wv['judge']) # vector for \"judge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4cfa64",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ef4cfa64",
    "outputId": "c15b3b98-70c9-4409-9e01-2b6b26d9641d"
   },
   "outputs": [],
   "source": [
    "print(w2v_model.wv.get_vector('law'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca1f7a2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dca1f7a2",
    "outputId": "c75b6f8f-a9a5-487c-e167-7621bdd1bccc"
   },
   "outputs": [],
   "source": [
    "## Cosine similarity between two vectors\n",
    "print(w2v_model.wv.similarity('crime', 'law'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa7be07",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aaa7be07",
    "outputId": "b24d2f6d-e178-4b7e-8229-454d2e1a0733"
   },
   "outputs": [],
   "source": [
    "## Most similar words\n",
    "w2v_model.wv.similar_by_word('crime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec26e77",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "id": "eec26e77",
    "outputId": "9ffa792c-9b81-4131-bbbb-b3d84af9d573"
   },
   "outputs": [],
   "source": [
    "## We can even see which words are not fitting in a given pattern\n",
    "w2v_model.wv.doesnt_match(\"he committed a crime horse with a weapon\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052cc998",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "052cc998",
    "outputId": "ecf5db00-6c76-4343-a527-c5852a625d27"
   },
   "outputs": [],
   "source": [
    "## vector addition - we can add vectors to get to a new \"vector\" (that might not exist)\n",
    "\n",
    "vector = w2v_model.wv.get_vector('corporation') - w2v_model.wv.get_vector('criminal') \n",
    "w2v_model.wv.similar_by_vector(vector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e189bd72",
   "metadata": {
    "id": "e189bd72"
   },
   "outputs": [],
   "source": [
    "vector = w2v_model.wv.get_vector('crime') + w2v_model.wv.get_vector('judge')  ## impeachment?\n",
    "w2v_model.wv.similar_by_vector(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b24af0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71b24af0",
    "outputId": "6331ce89-f469-4ce2-c893-729548f21ec2"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=['law', 'court'], negative = ['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed62eda",
   "metadata": {
    "id": "0ed62eda"
   },
   "source": [
    "## Visualizing word2vec word embeddings\n",
    "Once we have our word embedding model, we can viszualize it using the standard techniques - such as PCA and TSNE. \n",
    "\n",
    "The problem is that we're reducing from 100 dimensions to 2. Note that PCA has a unique singular representation whereas TSNE is a bit more complex, so it will always have a different representation every time you print the graph. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4czQsuv1WXZe",
   "metadata": {
    "id": "4czQsuv1WXZe"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This code is adapted from https://github.com/drelhaj/NLP_ML_Visualization_Tutorial/blob/master/6_Word_embeddings_Tutorial.ipynb\n",
    "def tsnescatterplot(model, word, list_names):\n",
    "    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n",
    "    its list of most similar words, and a list of words.\n",
    "    \"\"\"\n",
    "    arrays = np.empty((0, 100), dtype='f')\n",
    "    word_labels = [word]\n",
    "    color_list  = ['red']\n",
    "    # adds the vector of the query word\n",
    "    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n",
    "    \n",
    "    # gets list of most similar words\n",
    "    close_words = model.wv.most_similar([word])\n",
    "    \n",
    "    # adds the vector for each of the closest words to the array\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n",
    "        word_labels.append(wrd_score[0])\n",
    "        color_list.append('blue')\n",
    "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "    \n",
    "    # adds the vector for each of the words from list_names to the array\n",
    "    for wrd in list_names:\n",
    "        wrd_vector = model.wv.__getitem__([wrd])\n",
    "        word_labels.append(wrd)\n",
    "        color_list.append('green')\n",
    "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "        \n",
    "    # Reduces the dimensionality from 300 to 50 dimensions with PCA\n",
    "    reduc = PCA(n_components=20).fit_transform(arrays)\n",
    "    \n",
    "    # Finds t-SNE coordinates for 2 dimensions\n",
    "    np.set_printoptions(suppress=True)\n",
    "    \n",
    "    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)\n",
    "    \n",
    "    # Sets everything up to plot\n",
    "    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n",
    "                       'y': [y for y in Y[:, 1]],\n",
    "                       'words': word_labels,\n",
    "                       'color': color_list})\n",
    "    \n",
    "    fig = plt.subplots()\n",
    "    #fig.set_size_inches(9, 9)\n",
    "    \n",
    "    # Basic plot\n",
    "    p1 = sns.regplot(data=df,\n",
    "                     x=\"x\",\n",
    "                     y=\"y\",\n",
    "                     fit_reg=False,\n",
    "                     marker=\"o\",\n",
    "                     scatter_kws={'s': 40,\n",
    "                                  'facecolors': df['color']\n",
    "                                 }\n",
    "                    )\n",
    "    \n",
    "    # Adds annotations one by one with a loop\n",
    "    for line in range(0, df.shape[0]):\n",
    "         p1.text(df[\"x\"][line],\n",
    "                 df['y'][line],\n",
    "                 '  ' + df[\"words\"][line].title(),\n",
    "                 horizontalalignment='left',\n",
    "                 verticalalignment='bottom', size='medium',\n",
    "                 color=df['color'][line],\n",
    "                 weight='normal'\n",
    "                ).set_size(15)\n",
    "\n",
    "    \n",
    "    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n",
    "    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n",
    "            \n",
    "    plt.title('t-SNE visualization for {}'.format(word.title()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74jpBUmWcRK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "f74jpBUmWcRK",
    "outputId": "698a029f-f417-440f-ea75-37ae8fa2744a"
   },
   "outputs": [],
   "source": [
    "word = 'crime'\n",
    "tsnescatterplot(w2v_model, word,\n",
    "                [t[0] for t in w2v_model.wv.most_similar(positive=[word], \n",
    "                                                         topn=20)][10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc972651",
   "metadata": {},
   "source": [
    "### PCA plot of all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KchCR2lxZuEM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KchCR2lxZuEM",
    "outputId": "6e4d2e86-6c3d-46ac-915e-4d76d8972b71"
   },
   "outputs": [],
   "source": [
    "pca_df = pd.DataFrame(w2v_model.wv[w2v_model.wv.index_to_key], \n",
    "                      index=w2v_model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b1bfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0508feb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df_reduced = pca_df.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93174b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df_reduced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ce30ab",
   "metadata": {},
   "source": [
    "Full word dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I7qVRi3VZ8mn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "I7qVRi3VZ8mn",
    "outputId": "f8fa665e-7bef-4aa7-abac-6548e56fbdcf"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "components = pca.fit_transform(pca_df)\n",
    "plot = plt.scatter(components[:,0], components[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f37af34",
   "metadata": {},
   "source": [
    "### Use reduced word dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ev5pmlCxaepa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "Ev5pmlCxaepa",
    "outputId": "cab06304-4fb6-46b1-8ee0-d93fce965d0a"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.scatter_matrix(\n",
    "    components,\n",
    "    dimensions=range(2),\n",
    "    color=pca_df_reduced.index\n",
    ")\n",
    "fig.update_traces(diagonal_visible=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78993747",
   "metadata": {},
   "source": [
    "### Save word2vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28d180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model.save('w2v_model_vectors.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b820c57",
   "metadata": {
    "id": "6b820c57"
   },
   "source": [
    "## Part 3 - [Doc2Vec](https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html#sphx-glr-auto-examples-tutorials-run-doc2vec-lee-py)\n",
    "Doc2Vec is the same thing as word2vec, but with an extra representation [for a given document](https://miro.medium.com/max/640/0*x-gtU4UlO8FAsRvL.) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319fc971",
   "metadata": {
    "id": "319fc971"
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "docs = []\n",
    "for i, row in sample_df.iterrows():\n",
    "    docs += [word_tokenize(row['text'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9698b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n_Ph3_uaXLfz",
   "metadata": {
    "id": "n_Ph3_uaXLfz"
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "doc_iterator = [TaggedDocument(doc, [i]) for i, doc in enumerate(docs)]\n",
    "\n",
    "d2v_model = Doc2Vec(doc_iterator, # list of tokenized documents\n",
    "                   workers = 4, # Number of threads to run in parallel\n",
    "                   vector_size = 100,  # Word vector dimensionality     \n",
    "                   min_count = 2, # Minimum word count  \n",
    "                   window = 10 # Context window size      \n",
    "                   #max_vocab_size =  10000\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xzuoEACNXUW-",
   "metadata": {
    "id": "xzuoEACNXUW-"
   },
   "outputs": [],
   "source": [
    "# d2v_model.save('d2v-vectors.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7pBf9dJSXUjj",
   "metadata": {
    "id": "7pBf9dJSXUjj"
   },
   "outputs": [],
   "source": [
    "# matrix of all document vectors:\n",
    "doc2vec_matrix = d2v_model.dv.vectors\n",
    "doc2vec_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed59d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_matrix = pd.DataFrame(data = doc2vec_matrix, \n",
    "                          index = sample_df['name_abbreviation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a73841",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dzI9hAJCXUzt",
   "metadata": {
    "id": "dzI9hAJCXUzt"
   },
   "outputs": [],
   "source": [
    "#to find the vector of a document which is NOT in training data\n",
    "a = d2v_model.infer_vector(['the murder was committed by the defendant'])\n",
    "\n",
    "b = d2v_model.infer_vector(['the criminal assaulted the victim'])\n",
    "\n",
    "c = d2v_model.infer_vector(['the corporation is not able to pay its taxes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Zb5CZ75YXU-J",
   "metadata": {
    "id": "Zb5CZ75YXU-J"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(cosine_similarity(np.expand_dims(a, axis=0), \n",
    "                        np.expand_dims(b, axis=0)))\n",
    "print(cosine_similarity(np.expand_dims(a, axis=0), \n",
    "                        np.expand_dims(c, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2Jc7Fk_Cc0DC",
   "metadata": {
    "id": "2Jc7Fk_Cc0DC"
   },
   "outputs": [],
   "source": [
    "# get all pair-wise document similarities\n",
    "pairwise_sims = cosine_similarity(doc2vec_matrix)\n",
    "pairwise_sims.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258696cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_similarity_matrix = pd.DataFrame(data = pairwise_sims, \n",
    "                                  columns = sample_df['name_abbreviation'],\n",
    "                                  index = sample_df['name_abbreviation'])\n",
    "d2v_similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1a736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_similarity_matrix.sort_values(by='Silvers v. Jones', \n",
    "                          ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0e831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_of_interest = 'Getchell v. Rust' ## 2nd best match\n",
    "\n",
    "get_index(case_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5879f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_df['text'][get_index(case_of_interest)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4410603",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df['text'][4528]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e265b149",
   "metadata": {},
   "source": [
    "### We can also cluster documents\n",
    "\n",
    "See previous lab for different clustering methods and approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Mn3vQxz_c0Ec",
   "metadata": {
    "id": "Mn3vQxz_c0Ec"
   },
   "outputs": [],
   "source": [
    "# Document clusters\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# create 10 clusters of similar documents\n",
    "num_clusters = 4\n",
    "kmw = KMeans(n_clusters=num_clusters)\n",
    "kmw.fit(doc2vec_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40xPMaOCdCWW",
   "metadata": {
    "id": "40xPMaOCdCWW"
   },
   "outputs": [],
   "source": [
    "# Documents from an example cluster\n",
    "for i, doc in enumerate(docs):\n",
    "    if kmw.labels_[i] == 25:\n",
    "        print(' '.join(doc[:9]))\n",
    "    if i == 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L5UXB-EYdCu2",
   "metadata": {
    "id": "L5UXB-EYdCu2"
   },
   "outputs": [],
   "source": [
    "#%% PCA Viz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.scatter(Xpca[:,0],Xpca[:,1], alpha=.1)\n",
    "\n",
    "cdict = {1: 'red', 2: 'blue', 3: 'green'}\n",
    "fig, ax = plt.subplots()\n",
    "#for g, label in cdict.items():\n",
    "for g in np.unique(kmw.labels_):\n",
    "    ix = np.where(kmw.labels_ == g)\n",
    "    #ix = np.where(kmw == g)\n",
    "    #    ax.scatter(scatter_x[ix], scatter_y[ix], c = cdict[g], label = g, s = 100)\n",
    "    if g in cdict:\n",
    "        # use color from cdict\n",
    "        color = cdict[g]\n",
    "        ax.scatter(Xpca[:,0][ix], Xpca[:,1][ix], c = color, label = g, s = 100, alpha=1)\n",
    "    else:\n",
    "        if g < 10:\n",
    "            color = \"black\"\n",
    "            ax.scatter(Xpca[:,0][ix], Xpca[:,1][ix], c = color, label = g, s = 100, alpha=1)\n",
    "    \n",
    "\n",
    "        \n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7437cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_matrix_reduced = d2v_matrix.sample(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190694b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_matrix_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B4c4DfBIdC6d",
   "metadata": {
    "id": "B4c4DfBIdC6d"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "components = pca.fit_transform(d2v_matrix_reduced)\n",
    "\n",
    "fig = px.scatter_matrix(\n",
    "    components,\n",
    "    dimensions=range(2),\n",
    "    color=d2v_matrix_reduced.index\n",
    ")\n",
    "fig.update_traces(diagonal_visible=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34192116",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab 5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
