{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLNRq5qfNmw3"
   },
   "source": [
    "# Lab 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we'll be covering topic modeling - which moves us beyond the \"bag of words\" representation of text. Topic modeling (and related techniques)  can be thought of as a type of \"dimensionality reduction\" technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMoRgntIirUC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONLgMu6LXJl1"
   },
   "source": [
    "## Part 1: Create a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WnBjXBi8QFaM",
    "outputId": "91174e69-ec2e-428e-fdc4-d44d9c12b226"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import lzma\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from config import settings_base as settings\n",
    "from config import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJUmadY7QSCN"
   },
   "outputs": [],
   "source": [
    "# Get Case Data for California\n",
    "compressed_file = utils.get_and_extract_from_bulk(jurisdiction=\"Delaware\", \n",
    "                                                  data_format=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_hiBM3fSWfN"
   },
   "outputs": [],
   "source": [
    "# Assume we are dealing with json data (if data_format is changed to xml or\n",
    "# change this cell's os.path.join line)\n",
    "if not compressed_file.endswith('.xz'):\n",
    "  compressed_file = os.path.join(compressed_file, \"data\", \"data.jsonl.xz\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f64uAXVth1dd",
    "outputId": "49694f0c-869e-4d1e-b911-fb439f22ee9f"
   },
   "outputs": [],
   "source": [
    "cases = []\n",
    "print(\"File path:\", compressed_file)\n",
    "with lzma.open(compressed_file) as infile:\n",
    "    for line in infile:\n",
    "        record = json.loads(str(line, 'utf-8'))\n",
    "        cases.append(record)\n",
    "\n",
    "print(\"Case count: %s\" % len(cases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 635
    },
    "id": "eulgufhqh3QX",
    "outputId": "dd8a111a-cae9-4ec0-af61-1244f0acf68d"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(cases)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJDLEKa5bxPL"
   },
   "outputs": [],
   "source": [
    "opinion_data = []\n",
    "for case in cases:\n",
    "    for opinion in case[\"casebody\"][\"data\"][\"opinions\"]:\n",
    "        temp = {}\n",
    "        keys = list(case.keys())\n",
    "        keys.remove('casebody')\n",
    "        for key in keys:         \n",
    "            temp[key] = case[key]\n",
    "        keys = list(opinion.keys())\n",
    "        for key in keys:         \n",
    "            temp[key] = opinion[key]\n",
    "        opinion_data.append(temp)\n",
    "        \n",
    "df = pd.DataFrame(opinion_data)\n",
    "df[\"citations\"] = df[\"citations\"].apply(lambda x:x[0]['cite'])\n",
    "df[\"court\"] = df[\"court\"].apply(lambda x:x['name'])\n",
    "df[\"decision_date\"] = df[\"decision_date\"].apply(lambda x:int(x[:4]))\n",
    "df = df[df['court'] =='Delaware Court of Chancery']\n",
    "df = df.drop([\"docket_number\", \"first_page\", \n",
    "                                \"last_page\", \"name\",\n",
    "                                \"reporter\", \"volume\", \"jurisdiction\"], axis=1)\n",
    "df = df[[\"name_abbreviation\", \"decision_date\", \"court\", \"author\", \"type\", \"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "Ib_6-hF-EOtN",
    "outputId": "ad6404b6-823d-4530-8d6d-e7ebcd59f6a0"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4cxayzKl_g3"
   },
   "source": [
    "# Part 2: Clustering algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is an example of \"unsupervised\" machine learning - meaning that there is no human telling the computer the labels.\n",
    "\n",
    "Classification - that we studied last time - was \"supervised\" machine learning - meaning that we knew the \"labels\" of data beforehand - and these labels were used to create our classification models. \n",
    "\n",
    "Clustering is useful when we don't know the classes in our data. See: [this image](https://miro.medium.com/max/1400/0*Xe3YnRNqb7AuIUdu.gif) for a basic idea of \"clustering\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HbuvBBlpmIAx"
   },
   "source": [
    "### **K-means Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means is the granddaddy of clustering algorithms. Each data point (in this case, case) has to be a member of a distinct class. The \"within-class\" sum of squares has to be minimized - the  alogirthm tries to seperate clusters based on means of each \"cluster.\" \n",
    "\n",
    "The problem with K-means is the classsical problem of \"means\" - that each cluster is sensetive to outliers.\n",
    "\n",
    "See this image for a vizualization of the [iterative nature](https://media0.giphy.com/media/12vVAGkaqHUqCQ/giphy.gif?cid=790b76117b7e712fa0d37536e033c289c372b105a4d0447b&rid=giphy.gif&ct=g) of the K-means algorithm.\n",
    "\n",
    "See this image for a representation of how it works with [\"means\"](https://upload.wikimedia.org/wikipedia/commons/e/ea/K-means_convergence.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_B7rgwuHBssq"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=0.01,\n",
    "                             max_df=.9,  \n",
    "                             max_features=1000,\n",
    "                             stop_words='english',\n",
    "                            ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LcxtjK3iHJxt"
   },
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A5y3l4-FG1Wq",
    "outputId": "932d47c9-516c-489b-a4b3-dc2ca0c3e3da"
   },
   "outputs": [],
   "source": [
    "# Convert X to dense matrix, since the below clustering algorithms can only work with dense matrices\n",
    "X_dense = X.todense()\n",
    "X_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fQhSNockdQ0"
   },
   "outputs": [],
   "source": [
    "# create 10 clusters of similar documents\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 10\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "km.fit(X)\n",
    "doc_clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cluster'] = doc_clusters\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does each cluster represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['cluster'] == 3]['text'][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fE9JliJ2o3S6"
   },
   "source": [
    "**Silhouette Score**\n",
    "\n",
    "Sillhoutte score is one of those techniuqes which allows us to choose the \"optimal number\" of clusters for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ugk8egY6kdXp",
    "outputId": "9903c5a2-e1ed-4953-b4f4-b74f302a596e"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_score(X, km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLlhmjWPkdae"
   },
   "outputs": [],
   "source": [
    "sil_scores = []\n",
    "for n in range(2, num_clusters):\n",
    "    km = KMeans(n_clusters=n)\n",
    "    km.fit(X)\n",
    "    sil_scores.append(silhouette_score(X, km.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "VYIeCZcVkdda",
    "outputId": "b6c21fe4-8ef0-4cc6-bb1c-a8664d0af0e3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.plot(range(2, num_clusters), sil_scores)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JigwkOMxkdgc",
    "outputId": "3444db0d-834d-45a5-f040-1373671b0970"
   },
   "outputs": [],
   "source": [
    "opt_sil_score = max(sil_scores[0:10])\n",
    "sil_scores.index(opt_sil_score)\n",
    "opt_num_cluster = range(2, num_clusters)[sil_scores.index(opt_sil_score)]\n",
    "print('The optimal number of clusters is %s' %opt_num_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dlZv5UT_kdmd",
    "outputId": "f1c08075-5b8c-4aca-e298-3f6828d438f5"
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=opt_num_cluster)\n",
    "km.fit(X)\n",
    "doc_clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dlZv5UT_kdmd",
    "outputId": "f1c08075-5b8c-4aca-e298-3f6828d438f5"
   },
   "outputs": [],
   "source": [
    "df['cluster_mean'] = doc_clusters\n",
    "df[df['cluster_mean']==1]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9upKcyFqHqi"
   },
   "source": [
    "### **K-Medoids**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-medoid is similar to K-Means only that the difference is instead of means, we're working with centroids based on \"medians\" - which are less sensetive to outliers. \n",
    "\n",
    "Since medians have to be actual observations in the data, K-medoid allows us to pick an actual document which is, in a way most \"representative\" of a cluster. \n",
    "\n",
    "See [this picture](https://www.researchgate.net/publication/342871651/figure/fig1/AS:912165510864897@1594488613267/The-graphical-representation-of-the-difference-between-the-k-means-and-k-medoids.png) for an explanation of the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S9RTeWi3qTVI",
    "outputId": "7cdc18b4-c327-47c8-befe-104b35d12811"
   },
   "outputs": [],
   "source": [
    "!pip install scikit-learn-extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UOrMkNFjqO2X",
    "outputId": "4a153d54-1091-41f7-ec28-0121da519d50"
   },
   "outputs": [],
   "source": [
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "kmed = KMedoids(n_clusters=opt_num_cluster)\n",
    "kmed.fit(X)\n",
    "doc_clusters = kmed.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UOrMkNFjqO2X",
    "outputId": "4a153d54-1091-41f7-ec28-0121da519d50"
   },
   "outputs": [],
   "source": [
    "df['cluster_med'] = doc_clusters\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['cluster_med']==1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UOrMkNFjqO2X",
    "outputId": "4a153d54-1091-41f7-ec28-0121da519d50"
   },
   "outputs": [],
   "source": [
    "df[df['cluster_med']==1]['text'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['cluster_med']==1]['text'][18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6OhQ4gwq8dp"
   },
   "source": [
    "### **DBSCAN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN  is a relatively new clustering algorithm (first published in 2014) \n",
    " which stands for __\"Density-Based Spatial Clustering of Applications with Noise\"__. As the name suggests, DBSCAN uses \"density\" measure for points, rather than a simple means.\n",
    "\n",
    "To see how DBSCAN works based on densities - see [this image](https://ml-explained.com/articles/dbscan-explained/dbscan.gif)\n",
    "\n",
    "DBSCAN is very useful for non-circle/globular data - see [this image](https://miro.medium.com/max/1400/1*rfi9uHjGPdNgXgxe9xWvVw.png)\n",
    "\n",
    "There are also different determinations of what means [\"density\"](https://dashee87.github.io/images/DBSCAN_search.gif) - as anything outside the \"cluster\" is deemed to be noise (or outlier) depending on the parametarization.\n",
    "\n",
    "\n",
    "DBSCAN is a relatively new clustering algorithm (first published in 2014) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "brxr5mAXrE0E",
    "outputId": "48ada982-ef45-43d0-9ad6-74835023c10d"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=0.95, min_samples=5)\n",
    "dbscan.fit(X)\n",
    "db_clusters = dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "brxr5mAXrE0E",
    "outputId": "48ada982-ef45-43d0-9ad6-74835023c10d"
   },
   "outputs": [],
   "source": [
    "df['cluster_db'] = db_clusters\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cluster_db'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does __-1__ cluster mean? \n",
    "\n",
    "Check  the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) - __\"Cluster labels for each point in the dataset given to fit(). Noisy samples are given the label -1.\"__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "brxr5mAXrE0E",
    "outputId": "48ada982-ef45-43d0-9ad6-74835023c10d"
   },
   "outputs": [],
   "source": [
    "df[df['cluster_db']==1]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OD6QR3wkrSd8"
   },
   "source": [
    "### **Hierarchical DBSCAN**\n",
    "Automatically chooses epsilon, performing DBSCAN over various epsilon values - and returns the result that gives the best stability over epsilon. For reference see [here](https://github.com/scikit-learn-contrib/hdbscan/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iDxnf-kurjot",
    "outputId": "a7b5819e-6cd2-4c8c-841a-2f34225427ac"
   },
   "outputs": [],
   "source": [
    "#!pip install hdbscan\n",
    "#!pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BeoWXbMUraaz",
    "outputId": "0921940c-05a7-432e-b10b-7a5409189445"
   },
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "\n",
    "hdbscan = HDBSCAN(min_cluster_size=5)\n",
    "hdbscan.fit(X)\n",
    "hdb_clusters = hdbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BeoWXbMUraaz",
    "outputId": "0921940c-05a7-432e-b10b-7a5409189445"
   },
   "outputs": [],
   "source": [
    "df['cluster_hdb'] = hdb_clusters\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cluster_hdb'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BeoWXbMUraaz",
    "outputId": "0921940c-05a7-432e-b10b-7a5409189445"
   },
   "outputs": [],
   "source": [
    "df[df['cluster_hdb']==1]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNEndcbTrpK1"
   },
   "source": [
    "### **Hierarchical (Agglomerative) Clustering**\n",
    "For Agglomerative clustering, each point starts as a cluster, and is combined based on distance to other points. At the end - you get \"optimal\" number of clusters (depending on where you define the cutoff).\n",
    "\n",
    "See [this image](https://dashee87.github.io/images/hierarch.gif) for more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Kfk8Buwrv5V",
    "outputId": "36f6eb14-3fb9-4646-8e42-8b6ad657015a"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "cluster = AgglomerativeClustering(n_clusters=opt_num_cluster, \n",
    "                                  affinity='euclidean', \n",
    "                                  linkage='ward')\n",
    "\n",
    "cluster.fit_predict(X.toarray())\n",
    "\n",
    "clusters = dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Kfk8Buwrv5V",
    "outputId": "36f6eb14-3fb9-4646-8e42-8b6ad657015a"
   },
   "outputs": [],
   "source": [
    "df['cluster_hie'] = clusters\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cluster_hie'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Kfk8Buwrv5V",
    "outputId": "36f6eb14-3fb9-4646-8e42-8b6ad657015a"
   },
   "outputs": [],
   "source": [
    "df[df['cluster_hie']==1]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary - clustering is useful when we want to study how documents in our corpus are related. A set of documents could be a member of \"Cluster 1\" or \"Cluster 2\" - this can be useful.\n",
    "\n",
    "However, because each document __has__ to be a part of the cluster (or noise) - this makes the determination of clusters not that intuitive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Humans don't really think in terms of data points or \"similarity measures.\" We mostly think of things in terms of parts-based approaches - your face is not a bunch of points, but is a combination of \"the nose cluster\", \"eyes cluster,\" etc. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6FinSqIQGfH"
   },
   "source": [
    "## Part 3 - LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UA-Agx9UQRgX"
   },
   "source": [
    "Latent Dirilect Allocation, or LDA, is an approach to model the distribution of words that appear in a body of text.\n",
    "\n",
    "__\"Latent\"__ means hidden (nobody knows the exact topics in books)\n",
    "\n",
    "__\"Dirichlet\"__ means [Dirichlet probability disturbiton](https://en.wikipedia.org/wiki/Dirichlet_distribution)  used for this algorithm (mathy stuff\n",
    "\n",
    "__\"Allocation\"__ means allocating words to topics.\n",
    "\n",
    "Basically, one can think of the LDA algorith mas just allocation of words to topics based on the Dirichlet distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UA-Agx9UQRgX"
   },
   "source": [
    "* Unlike clustering, there is an assumption on the data - that \"words\" (data points) tend to appear together when certain topics are discussed. Thus, each topic \"generates\" a probability on the \"words\" that appear in it. \n",
    "\n",
    "* See [this image](https://miro.medium.com/max/800/1*pZo_IcxW1GVuH2vQKdoIMQ.jpeg) for an explanation of the intuition behind LDA\n",
    "\n",
    "* See [this image](https://miro.medium.com/max/1200/1*jjhkii_JmvCEazFAzQIdGA.gif) for a representation of LDA output details.\n",
    "\n",
    "* See [this video](https://youtu.be/MqPKguO5hDA) for a cool presentation of how LDA can be applied to other non-text domains.\n",
    "\n",
    "If you think about it carefully, everything in our lives actually a \"topic\". \n",
    "\n",
    "LDA is different from clustering in that it's \"generative\" - meaning there's a Dirichlet probability that relates  certain words appearing in a certain \"topic\". \n",
    "\n",
    "For example, the word \"Hogwarts\" (0.9 probability), \"magic\" (0.8 probability), \"wizardry\" (0.7 probability) appear in a topic on \"Harry Potter\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "JBIKnNIMRXd5",
    "outputId": "4936a207-5680-4941-af3d-d2c83f278be6"
   },
   "outputs": [],
   "source": [
    "# vizualize the document term matrix from Part 2\n",
    "X_matrix = pd.DataFrame(X_dense,\n",
    "                       columns = vectorizer.get_feature_names())\n",
    "X_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uuXCY2HrSYec",
    "outputId": "e2f4c8cf-cbd5-4d20-f2ac-541e79360270"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# n_topics represents the number of topics you're training the LDA model to fit to\n",
    "n_topics = 20\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components = n_topics, # how many topics we want \n",
    "                                max_iter = 10, # maximum learning iterations \n",
    "                                learning_method = 'online',\n",
    "                                learning_offset = 80., \n",
    "                                total_samples = len(X_matrix),\n",
    "                                random_state = 0)\n",
    "lda.fit(X_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NDXHQ5qIUxys",
    "outputId": "2f491146-caa2-4014-9bf3-3b3ecdfe503f"
   },
   "outputs": [],
   "source": [
    "#This is a function to print out the top words for each topic in a pretty way.\n",
    "#Don't worry too much about understanding every line of this code.\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# take a look at the print_top_words function above to get an understanding of what each paramter menas\n",
    "print_top_words(lda, feature_names, 15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the distribution array\n",
    "topic_dist = lda.transform(X)\n",
    "topic_dist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dist_df = pd.DataFrame(topic_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge back in with the original dataframe.\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "topic_dist_df = pd.DataFrame(topic_dist)\n",
    "\n",
    "df = df.reset_index() ## need to reset index to merge properly)\n",
    "df_w_topics = df.join(topic_dist_df)\n",
    "df_w_topics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What is the most \"representative topic\" of topic 8?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_of_interest = 8\n",
    "\n",
    "df_w_topics[['name_abbreviation', \n",
    "             'decision_date', \n",
    "             'text',\n",
    "              topic_of_interest]].sort_values(by=[topic_of_interest], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "FwIUjue7UCIG",
    "outputId": "85b4115a-127a-4ad9-d034-1da4a0920424"
   },
   "outputs": [],
   "source": [
    "!pip install pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "FwIUjue7UCIG",
    "outputId": "85b4115a-127a-4ad9-d034-1da4a0920424"
   },
   "outputs": [],
   "source": [
    "# pyLDAvis is a package which allows you to view topic distribution of your text\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "lda_display = pyLDAvis.sklearn.prepare(lda, \n",
    "                                       X, \n",
    "                                       vectorizer)\n",
    "\n",
    "pyLDAvis.save_html(lda_display, 'lda_visualization.html')\n",
    "# See lda_visualization.html to explore the LDA based topics\n",
    "\n",
    "lda_display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLFfEQuZLOVw"
   },
   "source": [
    "## Part 4 - NMF, PCA, SVD - matrix decomposition methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tvAipCuGz7m"
   },
   "source": [
    "### **Non-negative Matrix Factorization (NMF)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJz87oV-IqfO"
   },
   "source": [
    "Non-Negative Matrix Factorization (Lee and Seung, 1999) is a \"Matrix decomposition\" method with an imposed non-negativity constraint.     \n",
    "\n",
    "\n",
    "Recall from the classification lab, that the Document-Term Matrix is full of zeroes - but it has __no negative values__ meaning we can use non-negative matrix factorization. \n",
    "\n",
    "The intuition is that we define our Document Term Matrix (X) as being composed of 2 other matrices (W) and (H).\n",
    "\n",
    "Thus, [X ~= W * H](https://media.geeksforgeeks.org/wp-content/uploads/20210429213042/Intuition1-660x298.png)\n",
    "\n",
    "\n",
    "See [this image](https://blog.acolyer.org/wp-content/uploads/2019/02/nmf-fig-1.jpeg?w=640) for how NMF was used initially for image processing.\n",
    "\n",
    "See [this image](https://miro.medium.com/max/1400/1*Cdk8UXkHqkLxfPEFTNEU4A.jpeg) for details on the \"matrix decomposition\" part as it applies to text.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uSVDNS5ZIp_J",
    "outputId": "ce5b5682-f048-4a0f-ba4d-88cd9d0afdcc"
   },
   "outputs": [],
   "source": [
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "\n",
    "\n",
    "# Define get_descriptor function which will show top words for a given topic\n",
    "def get_descriptor( features, H, topic_index, top ):\n",
    "    top_indices = np.argsort( H[topic_index,:] )[::-1]\n",
    "    top_terms = []\n",
    "    for term_index in top_indices[0:top]:\n",
    "        top_terms.append( features[term_index] )\n",
    "    return top_terms\n",
    "\n",
    "# define get_top_documents function which will show us top cases associated with topics\n",
    "def get_top_documents( cases, W, topic_index, top ):\n",
    "    top_indices = np.argsort( W[:,topic_index] )[::-1]\n",
    "    top_documents = []\n",
    "    for doc_index in top_indices[0:top]:\n",
    "        top_documents.append(cases[doc_index])\n",
    "    return top_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6N3Lpj7C-hN",
    "outputId": "9baeb0a5-7a7f-44cf-ac82-e39ad0ac3b6c"
   },
   "outputs": [],
   "source": [
    "n_topics = 10\n",
    "top_words = 10\n",
    "\n",
    "# create NMF model\n",
    "from sklearn import decomposition\n",
    "model = decomposition.NMF(init = \"nndsvd\", \n",
    "                          n_components = n_topics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the model and extract the two W and H matrices -> X ~= W*H \n",
    "W = model.fit_transform(X)\n",
    "H = model.components_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show topics and words in those topics\n",
    "descriptors = []\n",
    "for topic_index in range( n_topics ):\n",
    "    descriptors.append( get_descriptor( vocab, H, topic_index, top_words) )  # Top 10 words\n",
    "    str_descriptor = \", \".join( descriptors[topic_index] )\n",
    "    print(\"Topic %02d: %s\" % ( topic_index+1, str_descriptor ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most representative documents for a given topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_names = df['name_abbreviation'].tolist()\n",
    "\n",
    "topic_of_interest = 2\n",
    "n_docs = 10\n",
    "\n",
    "#Print top documents for a given topic\n",
    "topic_documents = get_top_documents(case_names, W, topic_of_interest, n_docs) \n",
    "for i, doc in enumerate(topic_documents):\n",
    "    print(\"%02d. %s\" % ((i+1), doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3P-7sG3HCP9"
   },
   "source": [
    "### **SVD - singular value decomposition**\n",
    "\n",
    "[SVD](https://www.sharetechnote.com/image/EngMath_Matrix_SVD_01_2.png) is another matrix decomposition technique - unlike NMF it gives 3 matrices instead of 2. \n",
    "\n",
    "In text analytic practice, it's not used as often as LDA or NMF for topic modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_topics(a):\n",
    "    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]]\n",
    "    topic_words = ([top_words(t) for t in a])\n",
    "    return [' '.join(t) for t in topic_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8zc_OPtaHBox",
    "outputId": "edfee45c-0caa-42e5-d1e4-51a3b98e77a4"
   },
   "outputs": [],
   "source": [
    "U, s, Vh = np.linalg.svd(X_matrix, full_matrices=False)\n",
    "print(U.shape, s.shape, Vh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mxuLzlyqHBto",
    "outputId": "33ce8359-999d-4a02-c048-036366daa733"
   },
   "outputs": [],
   "source": [
    "num_top_words=8\n",
    "\n",
    "def show_topics(a):\n",
    "    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]]\n",
    "    topic_words = ([top_words(t) for t in a])\n",
    "    return [' '.join(t) for t in topic_words]\n",
    "\n",
    "show_topics(Vh[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYpiBMkUJZlf"
   },
   "source": [
    "### Extra: Vizualizations - **PCA (Principle Component Analysis)** , __T-SNE__\n",
    "\n",
    "Another classical matrix decomposition technique useful for summarizing high-dimensional data while preserving most information. PCA is more used in terms __visualizing__ the data\n",
    "\n",
    "\n",
    "PCA can help us reduce the \"dimensionality\" of the data by finding the dimensions which explain most of the data - these \"dimensions\" are called \"principal components (PC1 and PC2).\" \n",
    "\n",
    "See [this image](https://miro.medium.com/max/800/1*ZXhPoYQIn-Y8mxoUpz5Ayw.gif) for an explanation of how it basically works\n",
    "\n",
    "See this [this really good](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues) post explainig PCA\n",
    "\n",
    "__T-SNE__ is another visualization method - but it is kinda problematic - see [this article](https://distill.pub/2016/misread-tsne/)\n",
    "\n",
    "Nowadays, a newer algorithm called __UMAP__ is used more often for these types of visualizaitons - but agian, the problem of summarizing high-to-low dimensional representations is a [difficult one](https://miro.medium.com/max/1200/1*IHiZSKD019MrRmiyxaw2gg.gif). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E2yes1w6JZ1E",
    "outputId": "fc13f561-03b7-4d0b-eb9f-2298be84b6a7"
   },
   "outputs": [],
   "source": [
    "# Principal Components\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3,\n",
    "          svd_solver='randomized')\n",
    "\n",
    "Xpca = pca.fit_transform(X_matrix)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "KJ_yZFbvJabB",
    "outputId": "0bd75dd2-1715-458a-df1c-d692fe0c12a4"
   },
   "outputs": [],
   "source": [
    "# PCA Viz\n",
    "plt.scatter(Xpca[:,0],Xpca[:,1], alpha=.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "13Ycj-ytKA9k",
    "outputId": "d3f59aea-85ab-4b33-9ff2-f6eefd070050"
   },
   "outputs": [],
   "source": [
    "#%% PCA 3D Viz\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "Axes3D(plt.figure()).scatter(Xpca[:,0],Xpca[:,1], Xpca[:,2], alpha=.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GstnJv0lKBjO",
    "outputId": "14f68a7a-0cf4-4fa5-d80c-b89dcfb10644"
   },
   "outputs": [],
   "source": [
    "# make components (dimensions) to explain 95% of variance\n",
    "pca = PCA(n_components=.95)\n",
    "X95 = pca.fit_transform(X_matrix)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "926GLAGVKpJ8",
    "outputId": "0cf4edf9-1317-4033-83ea-e815c46057c4"
   },
   "outputs": [],
   "source": [
    "#%% MDS, Isomap, and T-SNE\n",
    "from sklearn.manifold import MDS, Isomap, TSNE\n",
    "mds = MDS(n_components=2)\n",
    "Xmds = mds.fit_transform(X.toarray()[:500,:200])\n",
    "Axes3D(plt.figure()).scatter(Xmds[:,0],Xmds[:,1], alpha=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "Kzfi24ARKugj",
    "outputId": "0bf15422-9b8e-4902-a841-4dc0b180757b"
   },
   "outputs": [],
   "source": [
    "#%% Isomap\n",
    "iso = Isomap(n_components=2)\n",
    "Xiso = iso.fit_transform(X[:500,:200])\n",
    "Axes3D(plt.figure()).scatter(Xiso[:,0],Xiso[:,1], alpha=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "iENycKQFKvWr",
    "outputId": "bcdf8708-688c-41f7-ea73-8aa45b304c3c"
   },
   "outputs": [],
   "source": [
    "#%% t-SNE\n",
    "tsne = TSNE(n_components=2, n_iter=250)\n",
    "Xtsne = tsne.fit_transform(X[:500,:200])\n",
    "Axes3D(plt.figure()).scatter(Xtsne[:,0],Xtsne[:,1], alpha=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "x9upKcyFqHqi",
    "b6OhQ4gwq8dp"
   ],
   "name": "lab4-ls190.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
