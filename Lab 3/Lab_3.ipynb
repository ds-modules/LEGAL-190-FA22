{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMBXniLty07a"
   },
   "source": [
    "# Lab 3 \n",
    "\n",
    " __case.law API, word frequencies, regex, concordances and collocations__\n",
    "\n",
    "In this lab, we'll be going over how to access the case.law data using an API. We'll also examine the dataset, introduce Regex, and go over concordances and collocations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-VUiGIw9IF8"
   },
   "source": [
    "# **Part 1 - Introduction to APIs and the case.law Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For LS190 , we'll be extensively working with the **[case.law](https://case.law/)** database - which is a database of **360 years of United States caselaw.** To access this data we'll need to develop a simple understanding of APIs.\n",
    "\n",
    "* API is an acronym for **Application Programming Interface.**  If you ask me, that's seems like a pretty vague and general term (unless you are a CS person who can explain what this means). The term itself most likely comes from the early days of computing. \n",
    "\n",
    "* Notwithstanding the confusing nature of the term, for the purposes of this class an **API allows us to interact with, access and download data from the case.law database.** This is why a **[case.law API KEY](https://case.law/docs/site_features/api)** becomes important - as this key allows us to download the case.law data. You can think of an API key as a magical phrase like \"Open Sesame\" - which lets you access a database where hidden treasures of data await!\n",
    "\n",
    "* The overwhelming amounts of data has made **APIs and API KEYS** an important means of accessing data. For example, there's the **[Twitter API](https://developer.twitter.com/en/docs/twitter-api)** if you want to study tweets. Or **[NYTimes API](https://developer.nytimes.com/apis)** if you want to study the New York Times Archive.\n",
    "\n",
    "The API for case.law is very-well documented and you can find examples of how to use the API by following the various **[jupyter notebooks they provided by the case.law team](https://github.com/harvard-lil/cap-examples)**. Honestly, these notebooks are excellent - __so check them out!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T04:19:14.466625Z",
     "start_time": "2022-09-06T04:19:13.968617Z"
    },
    "id": "Nmt87jZXgsuj"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import lzma\n",
    "import json\n",
    "\n",
    "from config import settings_base as settings # Notice that we have a \"config\" folder - which the \"settings\" script\n",
    "from config import utils                     # and also has the \"utils.py\" script - which contains utility codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we are importing a couple of libraries. \n",
    "* **`lzma`** allows us to decompress the case.law data\n",
    "* **`json`** allows us to access the *dictionary* data structure \n",
    "* **`config`** is a folder which contains **settings_base** python script. This script should contain your **API KEY** \n",
    " * Note that each API key is unique and different. Because these notebooks are published on github, we cannot share the API Key. This is why I ask you to get the API key from the kind folks at case.law as soon as possible.\n",
    "* Finally, **`utils`** is a python script which contains helper functions written by case.law folks which allow us to download their data.\n",
    "\n",
    "The examples code below is based on the **[example notebooks](https://github.com/harvard-lil/cap-examples)**  example notebooks  written by the wonderful people working at case.law. The case.law project is incredibly important as it allows us to access **huge amounts of case text data** without having to pay a subscription for services like LexisNexis or Westlaw. This is a wonderful example of data democratization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T04:19:34.968740Z",
     "start_time": "2022-09-06T04:19:34.963762Z"
    },
    "id": "fATB2Hdhsa3n"
   },
   "outputs": [],
   "source": [
    "# Get Case Data for Hawaii (as it's a small-ish jurisdiction)\n",
    "compressed_file = utils.get_and_extract_from_bulk(jurisdiction=\"Hawaii\", \n",
    "                                                  data_format=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is stored as a [jsonl](https://jsonlines.org/) file - which stands for \"json lines.\" A .JSON is itself a dictionary, and \"lines\" stands for the fact that each entry in the data is stored as a line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:37:35.872625Z",
     "start_time": "2022-09-05T05:37:35.857259Z"
    },
    "id": "HNuUBD8DXBnX"
   },
   "outputs": [],
   "source": [
    "# Assume we are dealing with json data (if data_format is changed to xml or\n",
    "# change this cell's os.path.join line)\n",
    "if not compressed_file.endswith('.xz'):\n",
    "    compressed_file = os.path.join(compressed_file, \n",
    "                                   \"data\", \n",
    "                                   \"data.jsonl.xz\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code makes sure that we can load the json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:39:34.255825Z",
     "start_time": "2022-09-05T05:39:30.187653Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q0NIKWEPsfIF",
    "outputId": "75b9a2e1-29cc-471e-aec2-170969a0b1bf"
   },
   "outputs": [],
   "source": [
    "cases = []\n",
    "print(\"File path:\", compressed_file)\n",
    "with lzma.open(compressed_file) as infile:\n",
    "    for line in infile:\n",
    "        record = json.loads(str(line, 'utf-8'))\n",
    "        cases.append(record)\n",
    "\n",
    "print(\"Case count: %s\" % len(cases))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a __list__ of all the cases. Examine the first entry below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:37:42.172564Z",
     "start_time": "2022-09-05T05:37:42.141596Z"
    }
   },
   "outputs": [],
   "source": [
    "cases[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    "If we want to **limit** the amount of cases we are interested in (for lack of memory or space), we can use the following code below - which will take the first 500 cases from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:38:03.468419Z",
     "start_time": "2022-09-05T05:38:03.327560Z"
    }
   },
   "outputs": [],
   "source": [
    "max_records = 500\n",
    "\n",
    "cases_reduced = []\n",
    "with lzma.open(compressed_file) as infile:\n",
    "    for count, line in enumerate(infile):           ## enumerate() allows us to count the iterations \n",
    "        record = json.loads(str(line, 'utf-8'))     ## in this case, we want 500 cases\n",
    "        cases_reduced.append(record)\n",
    "        if count == max_records - 1:\n",
    "            break\n",
    "\n",
    "print(\"Case count: %s\" % len(cases_reduced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting the data into a pandas datafarme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a list called **\"cases\"** which contains all the cases from Hawaii.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make it into a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:39:56.178494Z",
     "start_time": "2022-09-05T05:39:56.086286Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704
    },
    "id": "smiRy_M3siNq",
    "outputId": "877effb5-053c-404c-aed9-be2f61f2a7a4"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(cases)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have access to case.law data's, Hawaii dataset. Fascinating!\n",
    "\n",
    "Note, however, that we don't really see the \"text\" of the decisions. The actual **text** is contained within a dictionary in the column named **casebody.** So it's a dictionary within a dictionary - these data structures can get pretty complicated!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting to the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the data structure of **\"casebody\"** a little bit further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:39:58.568249Z",
     "start_time": "2022-09-05T05:39:58.552597Z"
    }
   },
   "outputs": [],
   "source": [
    "df['casebody'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write another for loop to extract the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:40:00.803690Z",
     "start_time": "2022-09-05T05:40:00.646551Z"
    }
   },
   "outputs": [],
   "source": [
    "opinion_texts = []\n",
    "for i in range(len(df)):\n",
    "    if df['casebody'][i]['data']['opinions']:\n",
    "        text = df['casebody'][i]['data']['opinions'][0]['text'] # .lower() to lowercase\n",
    "        opinion_texts.append(text)\n",
    "    else:\n",
    "        opinion_texts.append(\"No Text Found\") ## If no text is found, have a \"NAN\" entry - eg. df.loc[df['text'] == 'No Text Found']\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:40:01.290828Z",
     "start_time": "2022-09-05T05:40:01.275202Z"
    }
   },
   "outputs": [],
   "source": [
    "print(opinion_texts[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reinsert the \"opinion_texts\" list into the dataframe under the column __'text'__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:40:03.585902Z",
     "start_time": "2022-09-05T05:40:03.570280Z"
    }
   },
   "outputs": [],
   "source": [
    "df['text'] = opinion_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:40:04.041355Z",
     "start_time": "2022-09-05T05:40:04.003565Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop the columns which contain the **metadata** - ie data about data (like for example \"last page\") - and don't seem important at the moment (since we care only about text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:40:06.448873Z",
     "start_time": "2022-09-05T05:40:06.417575Z"
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned = df[['decision_date', 'name_abbreviation', 'text']] ## keep only these columns\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Part 2 -  Word frequency over time**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data - let's try looking at simple word frequencies over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we'll be using code from the [case.law API example codes](https://github.com/harvard-lil/cap-examples/blob/develop/ngrams/ngrams.ipynb) on n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's convert our \"Decision Date\" column into datetime format. \"Datetime\" Format allows us to work with \"numbers\" as datetime objects - ie, their months and years and days. This is convenient because dates are not like normal numbers - usually months end at 30, and restart to 1. So it's obvious that it needs its own heuristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:40:18.768193Z",
     "start_time": "2022-09-05T05:40:18.736508Z"
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned['decision_date'] = pd.to_datetime(df_cleaned[\"decision_date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's extract the year from our newly converted datetime column. Looking at \"words over year\" is a good simple way of seeing __general trends__ in the law and legal language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:40:19.693775Z",
     "start_time": "2022-09-05T05:40:19.678169Z"
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned['year'] = df_cleaned['decision_date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:40:21.139284Z",
     "start_time": "2022-09-05T05:40:21.108330Z"
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function called \"search_ngram\" which counts all the **occurrences** (or frequencies) of a given word over a given year. Thus, for example, if we cared about the word \"robbery\", how many times did it appear over time in the Hawaii dataset, and what were the trends of this word over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:40:24.225058Z",
     "start_time": "2022-09-05T05:40:24.209425Z"
    }
   },
   "outputs": [],
   "source": [
    "def search_ngram(ngram):\n",
    "    pairs = []\n",
    "    for year in df_cleaned[\"year\"].unique():                           ## list all unique years\n",
    "        temp = df_cleaned[df_cleaned[\"year\"] == year][\"text\"].tolist() ## extract all the text for a given year\n",
    "        temp = \" \".join(temp).lower()                                  ## make into a string via .join and lowercase\n",
    "        total_number_of_words = len(temp.split(\" \"))                   ## count the tokenized words - use for relative frequency\n",
    "        ngram_count = temp.count(ngram.lower())\n",
    "        pairs.append((year, \n",
    "                      ngram_count/total_number_of_words))              ## normalize ngram count by total word count\n",
    "   \n",
    "    return pd.DataFrame(pairs, columns=['Year', 'Normalized Frequency'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the function does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:40:30.478413Z",
     "start_time": "2022-09-05T05:40:26.947374Z"
    }
   },
   "outputs": [],
   "source": [
    "robbery = search_ngram(\"robbery\")\n",
    "robbery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:40:37.884067Z",
     "start_time": "2022-09-05T05:40:37.742955Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize = (15,8))\n",
    "sns.lineplot(data = robbery, \n",
    "             x = \"Year\", \n",
    "             y = \"Normalized Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try something else. Since the output of the search_ngram() function is a Pandas Dataframe, we can explicitly put it in plotting code below. \n",
    "\n",
    "A good example is the word \"computer\". The word itself never existed before computers were invented, so if we don't see its occurrence in the past, we can conclude that the code is capturing trends in the text data. This  can be thought of as a __\"sanity check\"__ - ie a simple hypothesis that we all know to be true that is captured by the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:40:51.834780Z",
     "start_time": "2022-09-05T05:40:48.143990Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,8))\n",
    "sns.lineplot(data = search_ngram('Computer'), \n",
    "             x = \"Year\", \n",
    "             y = \"Normalized Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finallly, let's try one more word. To make the code easily reproducible, you can create a \"word_of_interest\" string, which you input into the data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:41:00.242847Z",
     "start_time": "2022-09-05T05:40:56.578179Z"
    }
   },
   "outputs": [],
   "source": [
    "word_of_interest = \"Crown\"\n",
    "\n",
    "plt.figure(figsize = (15,8))\n",
    "sns.lineplot(data = search_ngram(word_of_interest), \n",
    "             x = \"Year\", \n",
    "             y = \"Normalized Frequency\").set(title = word_of_interest + \" normalized frequency over time\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this pattern of the word \"Crown\" represent? This seems very interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can explore the case.law data further here if you'd like or draw some more plots. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Again - I encourage you to check out the case.law **[example notebooks](https://github.com/harvard-lil/cap-examples)**. For instance, the **[Cartwright notebook - which shows who was Illinois' most prolific judge](https://github.com/harvard-lil/cap-examples/blob/develop/bulk_exploration/cartwright.ipynb)** is pretty fascinating!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOnadrCz_4SR"
   },
   "source": [
    "# **Part 3 - Introduction to  Regular Expressions (ReGex)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw above, word frequencies are a very powerful tool for examining trends in text data - despite its simplicity. This simplicity has the added benefit of being __intuitive in interpretation__\n",
    "\n",
    "Nevertheless, there are other methods we can use to examine text. The most (in)famous way of searching for patterns in text is REGEX, or regular expressions. \n",
    "\n",
    "This can be especially useful if we want to remove some \"bad patterns\" - boilerplate, useless headers or footers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ei3cPafKCCms"
   },
   "source": [
    "**Sets, Quantifiers, and Special Characters**\n",
    "\n",
    "Regex (regular expressions) is a very powerful tool to find patterns in text. One of the best ways to learn Regex is by using Regex 101 to practice matching words in a body of text.\n",
    "\n",
    "[RegexR](https://regexr.com/)\n",
    "\n",
    "[Regex101](https://regex101.com/)\n",
    "\n",
    "[Regex Reference Sheet](http://www.rexegg.com/regex-quickstart.html#ref)\n",
    "\n",
    "For example, say we had a text and we wanted to find every instance of a word within that text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:41:33.375415Z",
     "start_time": "2022-09-05T05:41:33.360067Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zBeFZsbJCJqK",
    "outputId": "42473dd9-c5d7-4043-a836-6bee70f9caaf"
   },
   "outputs": [],
   "source": [
    "import regex as re\n",
    "text = \"Samuel and I went down to the river yesterday! Samuel isn't a very good swimmer, though. Good thing our friend Ilya was there to help.\"\n",
    "\n",
    "# the findall() function finds every instance of a specified word pattern within a text\n",
    "re.findall(r'Samuel', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3RTPiIhCOi5"
   },
   "source": [
    "Let's say that instead of only wanting to find Samuel, we wanted to find every word in the text starting with 'Sa'. What would we do? Use pattern matching!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:41:34.240173Z",
     "start_time": "2022-09-05T05:41:34.208745Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kndfT3OeCNh9",
    "outputId": "312fffa7-d93f-486f-f7e5-d43e7595fd97"
   },
   "outputs": [],
   "source": [
    "re.findall(r'Sa[a-z]*', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLgX7o7ICU3p"
   },
   "source": [
    "You may be wondering what the [a-z] in the Sa[a-z] pattern means. This is called a **set** in regex. When characters are within a set, such as  [abcde], any one character will match. However, regex has a special rule where [a-z] means the same thing as [abcde...xyz].\n",
    "\n",
    "Here are some more:\n",
    "~~~ \n",
    "[0-9]        any numeric character\n",
    "[a-z]        any lowercase alphabetic character\n",
    "[A-Z]        any uppercase alphabetic character\n",
    "[aeiou]      any vowel (i.e. any character within the brackets)\n",
    "[0-9a-z]     to combine sets, list them one after another \n",
    "[^...]       exclude specific characters\n",
    "~~~\n",
    "\n",
    "\n",
    "You still may be wondering how the entirety of Sahit was able to be matched if only one character within [a-z] would match. The answer is something called a **quantifier**!\n",
    "\n",
    "Rules:\n",
    "~~~ \n",
    "*        0 or more of the preceding character/expression\n",
    "+        1 or more of the preceding character/expression\n",
    "?        0 or 1 of the preceding character/expression\n",
    "{n}      n copies of the preceding character/expression \n",
    "{n,m}    n to m copies of the preceding character/expression \n",
    "~~~\n",
    "\n",
    "Say that now, you only wanted to return Samuel when the name was mentioned at the beginning of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:41:37.322414Z",
     "start_time": "2022-09-05T05:41:37.306838Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8BN1ZQwqCV7J",
    "outputId": "7f38c366-58e2-440b-85f3-9c53496fc486"
   },
   "outputs": [],
   "source": [
    "re.findall(r'^Samuel', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZMLtpFaCa-4"
   },
   "source": [
    "**Special characters**, such as the *^* which was just used in the pattern above, match strings that have a specific placement in a sentence. For example, *^* matches the subsequent pattern only if it is at the beginning of the string. This is why only a single 'Samuel' was returned.\n",
    "\n",
    "Rules:\n",
    "~~~ \n",
    ".         any single character except newline character\n",
    "^         start of string\n",
    "$         end of entire string\n",
    "\\n        new line\n",
    "\\r        carriage return\n",
    "\\t        tab\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ojtbv71XCojp"
   },
   "source": [
    "**Python RegEx Methods**\n",
    "\n",
    "* `re.findall(pattern, string)`: Returns all phrases that match your pattern in the string.\n",
    "\n",
    "* `re.sub(pattern, replacement, string)`: Return the string after replacing the leftmost non-overlapping occurrences of the pattern in string with replacement\n",
    "\n",
    "* `re.split(pattern, string)`: Split string by the occurrences of pattern. If capturing parentheses are used in pattern, then the text of all groups in the pattern are also returned as part of the resulting list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pandas RegEx Methods**\n",
    "\n",
    "Pandas also has its own [__built in methods__](https://kanoki.org/2019/11/12/how-to-use-regex-in-pandas/) of working with regex (without calling a separate \"re\" library)\n",
    "\n",
    "* `df['column'].str.extract(pattern)`: Extract pattern into a new column\n",
    "\n",
    "* `df['column'].str.replace(pattern, replace):` Replace pattern with something else (usually a \"space\" if you want to remove\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T08:47:54.404282Z",
     "start_time": "2022-09-04T08:47:54.389289Z"
    }
   },
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Regex cleaning example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to our case.law dataset and try to \"clean\" it of some repetitions in the data. \n",
    "\n",
    "For example - the __\"OPINION OF THE COURT\"__ in the beginning of every decision is not really useful information. \n",
    "\n",
    "**How would we use Regex to clean it up?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:42:22.682863Z",
     "start_time": "2022-09-05T05:42:22.667238Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:42:24.098066Z",
     "start_time": "2022-09-05T05:42:24.082442Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:42:25.922587Z",
     "start_time": "2022-09-05T05:42:25.891109Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df.text[200][:1000]) \n",
    "print(\"\\n\")\n",
    "print(df.text[300][:1000])\n",
    "print(\"\\n\")\n",
    "print(df.text[1][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, sometimes it's written as \"OPINION __OP__ THE COURT\" with a P, sometimes it's capitalized and sometimes it isn't. \n",
    "\n",
    "In my experience, the easiest way to work with these patterns is to copy them to [__RegexR__](https://regexr.com/), and work there on a small sample (like the one above). \n",
    "\n",
    "The important thing to note is that you must **search for patterns** in the text that will enable you to clean up the data. But __be very careful__ because REGEX can be very powerful - making an incorrect pattern could remove text that you didn't want to lose. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Try - an obvious pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing out some patterns, the simplest pattern we could use is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:42:31.234512Z",
     "start_time": "2022-09-05T05:42:31.046576Z"
    }
   },
   "outputs": [],
   "source": [
    "pattern = r'(Opinion of the Court|OPINION OP THE COURT BY|OPINION OF THE COURT BY)' # remember, in python we need the paranthesis\n",
    " \n",
    "df['pattern'] = df['text'].str.extract(pattern, expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:42:32.304853Z",
     "start_time": "2022-09-05T05:42:32.273580Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And actually... it's pretty good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conditionally check the ones that \"weren't captured\" - ie **NaN** - by subsetting the dataframe. \n",
    "\n",
    "We do that by creating a new dataframe which has the condition of having only the rows that contain **NaN** in the \"cleaned_text\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:42:58.790439Z",
     "start_time": "2022-09-05T05:42:58.759192Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test = df[df['pattern'].isna()]\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it's not really capturing all the useless text - see row 18220 for instance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second try "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another pattern that emerges is that the first line of the text is usually useless. The first line is seperated by a \"/n\" symbol. We could try that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:43:05.331386Z",
     "start_time": "2022-09-05T05:43:05.268830Z"
    }
   },
   "outputs": [],
   "source": [
    "pattern = r\"(^[\\s\\S]+?(?=\\n))\"\n",
    "\n",
    "df['pattern2'] = df['text'].str.extract(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:43:07.454703Z",
     "start_time": "2022-09-05T05:43:07.423210Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, not a perfect pattern. We are losing some information by capturing the names of Judges in the pattern - and thus we could remove that. \n",
    "\n",
    "But this is just an example. Now that we tested the pattern using `.str.extract()`, we can proceed to remove the pattern using `.str.replace()` by replacing the pattern with a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:43:15.897734Z",
     "start_time": "2022-09-05T05:43:13.700637Z"
    }
   },
   "outputs": [],
   "source": [
    "df['cleaned_text'] = df['text'].str.replace(pattern, \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:43:15.913766Z",
     "start_time": "2022-09-05T05:43:15.897734Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYqg1kc1Iv8j"
   },
   "source": [
    "# **Part 4 - Concordances and Collocations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igkvUNygJVSq"
   },
   "source": [
    "## Concordances\n",
    "To continue this examination of simple NLP methods, let us now look at concordances.\n",
    "\n",
    "A concordance lists every instance of a given word, together with some of its context. Concordances are __fundamentally important__ if we want to understand the meaning of a word in a context.\n",
    "\n",
    "Here we look up the word \"petition\" in casebody of the California Dataset by entering text followed by a period, then the term concordance, and then placing “petition” in parentheses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do concordances, we'll import __NLTK__, which is a good simple library for doing NLP tasks in Python. You will notice that we will be importing a lot of packages - which is just a way of showing that the NLP space in python is incredibly diverse and there are numerous libraries which can do a lot of different things. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:43:28.355451Z",
     "start_time": "2022-09-05T05:43:27.382541Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qq7Oag4tJGaU",
    "outputId": "e79058b0-2b2f-4ec2-c858-6b384e1cd6a8"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.text import Text\n",
    "from nltk.tokenize import word_tokenize # import tokenizer function from nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:43:30.775581Z",
     "start_time": "2022-09-05T05:43:30.759942Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:43:47.025424Z",
     "start_time": "2022-09-05T05:43:46.993766Z"
    }
   },
   "outputs": [],
   "source": [
    "cases = df['cleaned_text'][:100]  ## Make a list of cases out of the first 100 cases from the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T09:42:37.538589Z",
     "start_time": "2022-09-04T09:42:37.507603Z"
    }
   },
   "source": [
    "Now, we must convert these cases into a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:43:49.965155Z",
     "start_time": "2022-09-05T05:43:49.949530Z"
    }
   },
   "outputs": [],
   "source": [
    "case_corpus = \" \".join(cases).lower()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:43:53.188539Z",
     "start_time": "2022-09-05T05:43:53.172914Z"
    }
   },
   "outputs": [],
   "source": [
    "case_corpus[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we tokenize - more on this in the next lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:43:57.433642Z",
     "start_time": "2022-09-05T05:43:56.339159Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JM0A4YP_JKj_",
    "outputId": "1c41ff03-265c-4b71-b2f9-c10fb483a23a"
   },
   "outputs": [],
   "source": [
    "case_corpus_tokenized = word_tokenize(case_corpus)\n",
    "text = Text(case_corpus_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:43:58.282983Z",
     "start_time": "2022-09-05T05:43:58.141491Z"
    }
   },
   "outputs": [],
   "source": [
    "text.concordance('fraud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the word \"crown\" was used a lot in the past in Hawaii. Why is that? \n",
    "\n",
    "Let's examine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:45:34.778671Z",
     "start_time": "2022-09-05T05:45:34.763036Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JM0A4YP_JKj_",
    "outputId": "1c41ff03-265c-4b71-b2f9-c10fb483a23a"
   },
   "outputs": [],
   "source": [
    "text.concordance(\"crown\", \n",
    "                 width=110) ## width determines how many characters before and after we want to examine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:46:05.013469Z",
     "start_time": "2022-09-05T05:46:04.982477Z"
    }
   },
   "outputs": [],
   "source": [
    "## if we want phrases we need to put them in a list\n",
    "text.concordance([\"good\", \"faith\"],  \n",
    "                 width=110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEAq9YRTjFHH"
   },
   "source": [
    "**Write your own code to explore the occurrence of other words of interest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HtPjYeCwjGWQ",
    "outputId": "10d17730-2e86-498d-fcbb-62268d4f4555"
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTR1I6NpaxZt"
   },
   "source": [
    "## Collocations\n",
    "Collocations are expressions of multiple words which commonly co-occur. These collocations are measured using **Pointwise Mutual Information** - which gives the probability of \"two events co-occuring\" - in this case, two words co-occuring. This can give us measures of associations between words - things like phrases, or co-occuring words can be revealed.\n",
    "\n",
    "[A pretty good explanation of PMI](https://stats.stackexchange.com/a/522504) is given on **stackexchange.**\n",
    "\n",
    "Note: stackexchange (and google generally) is a wonderful resource for all things relating to NLP and statistics. Although for this course, I do not emphasize concrete statistical knowledge or mathematical formulas, you should still get some **intuitive** understanding of what these measures like PMI do. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:46:52.721741Z",
     "start_time": "2022-09-05T05:46:52.706127Z"
    },
    "id": "AWJNDIOIJNED"
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "fourgram_measures = nltk.collocations.QuadgramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:46:57.236189Z",
     "start_time": "2022-09-05T05:46:56.121911Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dSsFAHq2JPtl",
    "outputId": "78ee1dac-e3bc-4675-df13-2179bce20cfb"
   },
   "outputs": [],
   "source": [
    "bigram_finder = BigramCollocationFinder.from_words(case_corpus_tokenized,\n",
    "                                                   window_size = 5)\n",
    "\n",
    "bigram_finder.apply_freq_filter(5)  # appear at least N times\n",
    "bigram_finder.nbest(bigram_measures.pmi, 20) # show top 20 PMI scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T06:04:09.093547Z",
     "start_time": "2022-09-05T06:04:09.062563Z"
    }
   },
   "source": [
    "Do you see any interesting patterns that emerge from the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also examine the **score of these bigrams as they are in PMI**. \n",
    "\n",
    "The [mathematics](https://en.wikipedia.org/wiki/Pointwise_mutual_information) are not that important for the purposes of the class - as we care more about intuition behind these measures.\n",
    "\n",
    "It suffices to say that:\n",
    "* A bigger **positive PMI** score implies that a word1 (event1) tends to co-occur more with word2 (event2). \n",
    "* A **PMI score of 0** means that the two words (events) are independent. \n",
    "* A **negative PMI** score can mean that the two words are uninformative. In practice, usually, [Positive PMI (or __PPMI__)](https://stats.stackexchange.com/a/284573) is used (where the negative values are not included).\n",
    "\n",
    "To learn more about this, see generally Jurafsky and Martin referecen text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T05:48:07.675484Z",
     "start_time": "2022-09-05T05:48:05.713374Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in bigram_finder.score_ngrams(bigram_measures.pmi):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean the text a bit - remove numbers for example, perpahs this will reveal more patterns? \n",
    "\n",
    "__We'll get more into \"preprocessing raw text\" in our next lab__ - think of this as a simple introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T06:06:17.370245Z",
     "start_time": "2022-09-05T06:06:15.973486Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "\n",
    "case_corpus_tokenized = word_tokenize(strip_numeric(case_corpus)) ## we added the strip_numeric function\n",
    "text = Text(case_corpus_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T06:06:24.114816Z",
     "start_time": "2022-09-05T06:06:21.870642Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram_finder = BigramCollocationFinder.from_words(case_corpus_tokenized, \n",
    "                                                   window_size = 10)\n",
    "\n",
    "bigram_finder.apply_freq_filter(5) \n",
    "bigram_finder.nbest(bigram_measures.pmi, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pu7RXh6pjW_t"
   },
   "source": [
    "Let's try it with **Trigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T06:07:20.410050Z",
     "start_time": "2022-09-05T06:07:16.391138Z"
    },
    "id": "lk2b-A0qJSRh"
   },
   "outputs": [],
   "source": [
    "# Trigram\n",
    "trigram_finder = TrigramCollocationFinder.from_words(case_corpus_tokenized,\n",
    "                                                       window_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T06:07:21.888387Z",
     "start_time": "2022-09-05T06:07:21.574514Z"
    }
   },
   "outputs": [],
   "source": [
    "trigram_finder.apply_freq_filter(5) \n",
    "trigram_finder.nbest(trigram_measures.pmi, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try doing this on **fourgrams** on your own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aq0OZCEHDnCH"
   },
   "outputs": [],
   "source": [
    "# FourGram\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrasemachine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Phrasemachine](https://github.com/slanglab/phrasemachine) is another convenient way of finding multi-word expressions. Again, the mathematics of it is not important - what's important is the functionality and how to apply it in python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with \"Phrasemachine\" we first will have to import Spacy - which is a library with very powerful NLP tools - more on it in subsequent labs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T06:13:02.270044Z",
     "start_time": "2022-09-05T06:12:51.722399Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import phrasemachine\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy is incredibly powerful as it allows us to examine all the linguistic aspects of a given string. We'll cover this more in the upcoming lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T06:17:08.349335Z",
     "start_time": "2022-09-05T06:17:08.333722Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, \n",
    "          token.lemma_, \n",
    "          token.pos_) \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we see the power of Spacy, we can apply it to our corpus.\n",
    "\n",
    "In order to run the phrasemachine, we need to **tokenize** the case_corpus text first. Tokenization is essentially a process of demarcating a \"string\" into seperate \"tokens\". \n",
    "\n",
    "* For example - __\"this string needs to be tokenized\"__\n",
    "* Will become - **[\"this\", \"string\", \"needs\", \"to\", \"be\", \"tokenized\"]**\n",
    "\n",
    "More on this in the next lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T06:19:07.144953Z",
     "start_time": "2022-09-05T06:19:05.291815Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a spacy NLP pipeline\n",
    "doc = nlp(case_corpus[:100000])        ## limit our text to 100000 characters - otherwise it will take too long      \n",
    "tokens = [token.text for token in doc] ## tokenize\n",
    "pos = [token.pos_ for token in doc]    ## tag parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T06:20:02.198309Z",
     "start_time": "2022-09-05T06:20:02.167327Z"
    }
   },
   "outputs": [],
   "source": [
    "print(case_corpus[:128])\n",
    "print(tokens[:20])\n",
    "print(pos[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T06:22:51.199541Z",
     "start_time": "2022-09-05T06:22:51.058179Z"
    }
   },
   "outputs": [],
   "source": [
    "phrases = phrasemachine.get_phrases(tokens=tokens, \n",
    "                                    postags=pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Notice the data structure of the \"phrases\" - it's a __dictionary__. \n",
    "\n",
    "The \"counts\" key is itself a dictionary known as a [__\"counter class\"__](https://docs.python.org/3/library/collections.html#collections.Counter) - which counts the frequency of an item as its \"value\" in the \"key-value\" pair meaning of the term. \n",
    "\n",
    "Because it's a \"counter\" we can apply the \".most_common\" method on this \"counter\" to see the most common phrases that emerge. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T06:23:36.916067Z",
     "start_time": "2022-09-05T06:23:36.900017Z"
    }
   },
   "outputs": [],
   "source": [
    "phrases['counts'].most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note: in order to count words using the \"counter\" class, we had to tokenize our text. Again, we will cover more on this in the next lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab1 : Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "195px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
